{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6980e0e6",
   "metadata": {},
   "source": [
    "# DeepRoof Checkpoint Inference (PNG/JPG)\n",
    "\n",
    "Notebook validates the checkpoint, loads the DeepRoof model, runs segmentation on one image, and saves visual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_project_root() -> Path:\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path('/workspace/roof'),\n",
    "        Path('/Users/voskan/Desktop/DeepRoof-2026'),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if (c / 'configs').exists() and (c / 'deeproof').exists():\n",
    "            return c\n",
    "    raise FileNotFoundError('Could not auto-detect project root with configs/ and deeproof/')\n",
    "\n",
    "PROJECT_ROOT = detect_project_root()\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'deeproof_scratch_swin_L.py'\n",
    "\n",
    "# Prefer server checkpoint path; keep local fallback only for analysis/debug.\n",
    "SERVER_CHECKPOINT_PATH = Path('/workspace/roof/work_dirs/swin_l_scratch_v1/iter_8000.pth')\n",
    "LOCAL_ANALYSIS_CHECKPOINT_PATH = Path('/Users/voskan/Downloads/iter_8000.pth')\n",
    "CHECKPOINT_PATH = SERVER_CHECKPOINT_PATH if SERVER_CHECKPOINT_PATH.exists() else LOCAL_ANALYSIS_CHECKPOINT_PATH\n",
    "\n",
    "# Set your test image here (PNG/JPG/TIF).\n",
    "INPUT_IMAGE_PATH = Path('/workspace/test.png')\n",
    "if not INPUT_IMAGE_PATH.exists():\n",
    "    fallback = PROJECT_ROOT / 'test.png'\n",
    "    if fallback.exists():\n",
    "        INPUT_IMAGE_PATH = fallback\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs' / 'checkpoint_inference'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OVERLAY_PATH = OUTPUT_DIR / 'test_segmentation_overlay.png'\n",
    "SEM_MASK_PATH = OUTPUT_DIR / 'test_semantic_mask.png'\n",
    "SUMMARY_PATH = OUTPUT_DIR / 'test_inference_summary.json'\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'PROJECT_ROOT: {PROJECT_ROOT}')\n",
    "print(f'CONFIG: {CONFIG_PATH}')\n",
    "print(f'CHECKPOINT: {CHECKPOINT_PATH}')\n",
    "print(f'INPUT: {INPUT_IMAGE_PATH}')\n",
    "print(f'DEVICE: {DEVICE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47795d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in (CONFIG_PATH, CHECKPOINT_PATH, INPUT_IMAGE_PATH):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f'Path not found: {p}')\n",
    "\n",
    "img_bgr = cv2.imread(str(INPUT_IMAGE_PATH), cv2.IMREAD_COLOR)\n",
    "if img_bgr is None:\n",
    "    raise RuntimeError(f'Could not load image: {INPUT_IMAGE_PATH}')\n",
    "\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "H, W = img_rgb.shape[:2]\n",
    "print(f'Loaded image: {W}x{H}')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title('Input image')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint compatibility inspection\n",
    "# Required for PyTorch>=2.6 when old MMEngine checkpoints contain non-tensor objects.\n",
    "os.environ.setdefault('TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD', '1')\n",
    "\n",
    "def inspect_checkpoint(path: Path):\n",
    "    try:\n",
    "        ckpt = torch.load(str(path), map_location='cpu', weights_only=False)\n",
    "    except TypeError:\n",
    "        # Older torch versions without weights_only argument\n",
    "        ckpt = torch.load(str(path), map_location='cpu')\n",
    "\n",
    "    if not isinstance(ckpt, dict):\n",
    "        return {'type': str(type(ckpt)), 'error': 'checkpoint is not a dict'}\n",
    "\n",
    "    state_dict = ckpt.get('state_dict', ckpt.get('model', None))\n",
    "    info = {\n",
    "        'top_keys': list(ckpt.keys()),\n",
    "        'has_state_dict': isinstance(state_dict, dict),\n",
    "        'meta_keys': list(ckpt.get('meta', {}).keys()) if isinstance(ckpt.get('meta', None), dict) else [],\n",
    "    }\n",
    "\n",
    "    if isinstance(state_dict, dict):\n",
    "        keys = list(state_dict.keys())\n",
    "        info['num_params'] = len(keys)\n",
    "        info['first_keys'] = keys[:15]\n",
    "        probes = [\n",
    "            'backbone.patch_embed.projection.weight',\n",
    "            'decode_head.query_embed.weight',\n",
    "            'geometry_head.layers.0.weight',\n",
    "            'module.backbone.patch_embed.projection.weight',\n",
    "        ]\n",
    "        info['probe_hits'] = {k: (k in state_dict) for k in probes}\n",
    "\n",
    "    return info\n",
    "\n",
    "ckpt_info = inspect_checkpoint(CHECKPOINT_PATH)\n",
    "print(json.dumps(ckpt_info, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe0695",
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from mmseg.utils import register_all_modules\n",
    "from mmseg.apis import init_model, inference_model\n",
    "from mmengine.config import ConfigDict\n",
    "\n",
    "register_all_modules(init_default_scope=False)\n",
    "\n",
    "# Ensure custom modules are imported and registered.\n",
    "import deeproof.models.backbones.swin_v2_compat\n",
    "import deeproof.models.deeproof_model\n",
    "import deeproof.models.heads.mask2former_head\n",
    "import deeproof.models.heads.geometry_head\n",
    "import deeproof.models.losses\n",
    "\n",
    "def _ensure_test_pipeline(cfg):\n",
    "    if hasattr(cfg, 'test_pipeline') and cfg.test_pipeline:\n",
    "        return\n",
    "\n",
    "    pipeline = None\n",
    "    for loader_key in ('test_dataloader', 'val_dataloader', 'train_dataloader'):\n",
    "        if loader_key in cfg and cfg.get(loader_key) is not None:\n",
    "            dataset_cfg = cfg[loader_key].get('dataset', None)\n",
    "            if dataset_cfg is not None:\n",
    "                candidate = dataset_cfg.get('pipeline', None)\n",
    "                if candidate:\n",
    "                    pipeline = list(candidate)\n",
    "                    break\n",
    "\n",
    "    if not pipeline:\n",
    "        pipeline = [dict(type='LoadImageFromFile'), dict(type='PackSegInputs')]\n",
    "\n",
    "    filtered = []\n",
    "    has_load_image = False\n",
    "    has_pack_inputs = False\n",
    "    for transform in pipeline:\n",
    "        if not isinstance(transform, dict):\n",
    "            continue\n",
    "        t = transform.get('type', '')\n",
    "        if t == 'LoadAnnotations':\n",
    "            continue\n",
    "        if t == 'LoadImageFromFile':\n",
    "            has_load_image = True\n",
    "        if t == 'PackSegInputs':\n",
    "            has_pack_inputs = True\n",
    "        filtered.append(transform)\n",
    "\n",
    "    if not has_load_image:\n",
    "        filtered.insert(0, dict(type='LoadImageFromFile'))\n",
    "    if not has_pack_inputs:\n",
    "        filtered.append(dict(type='PackSegInputs'))\n",
    "\n",
    "    cfg.test_pipeline = filtered\n",
    "\n",
    "model = init_model(str(CONFIG_PATH), str(CHECKPOINT_PATH), device=DEVICE)\n",
    "_ensure_test_pipeline(model.cfg)\n",
    "\n",
    "# Fix inference crash when model.test_cfg is missing or wrong type in custom training configs.\n",
    "if getattr(model, 'test_cfg', None) is None:\n",
    "    model.test_cfg = ConfigDict(mode='whole')\n",
    "elif isinstance(model.test_cfg, dict):\n",
    "    cfg_tmp = dict(model.test_cfg)\n",
    "    cfg_tmp.setdefault('mode', 'whole')\n",
    "    model.test_cfg = ConfigDict(cfg_tmp)\n",
    "else:\n",
    "    # Some versions expect attr-access (test_cfg.mode), others use get().\n",
    "    # Ensure 'mode' exists for both behaviors.\n",
    "    if not hasattr(model.test_cfg, 'mode'):\n",
    "        try:\n",
    "            setattr(model.test_cfg, 'mode', 'whole')\n",
    "        except Exception:\n",
    "            model.test_cfg = ConfigDict(mode='whole')\n",
    "\n",
    "model.eval()\n",
    "print('Model loaded successfully.')\n",
    "print('test_pipeline:', model.cfg.test_pipeline)\n",
    "print('model.test_cfg:', model.test_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = inference_model(model, str(INPUT_IMAGE_PATH))\n",
    "if isinstance(result, (list, tuple)):\n",
    "    result = result[0]\n",
    "\n",
    "# Semantic map\n",
    "if hasattr(result, 'pred_sem_seg') and hasattr(result.pred_sem_seg, 'data'):\n",
    "    sem_map = result.pred_sem_seg.data.squeeze(0).detach().cpu().numpy().astype(np.uint8)\n",
    "else:\n",
    "    sem_map = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "if sem_map.shape != (H, W):\n",
    "    sem_map = cv2.resize(sem_map, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Instances (if available)\n",
    "masks = np.zeros((0, H, W), dtype=bool)\n",
    "scores = np.array([], dtype=np.float32)\n",
    "labels = np.array([], dtype=np.int64)\n",
    "\n",
    "if hasattr(result, 'pred_instances') and result.pred_instances is not None:\n",
    "    inst = result.pred_instances\n",
    "    if hasattr(inst, 'masks') and inst.masks is not None:\n",
    "        masks_t = inst.masks\n",
    "        if torch.is_tensor(masks_t):\n",
    "            masks_np = masks_t.detach().cpu().numpy().astype(bool)\n",
    "            if masks_np.ndim == 3:\n",
    "                resized_masks = []\n",
    "                for m in masks_np:\n",
    "                    if m.shape != (H, W):\n",
    "                        m = cv2.resize(m.astype(np.uint8), (W, H), interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "                    resized_masks.append(m)\n",
    "                masks = np.stack(resized_masks, axis=0) if resized_masks else np.zeros((0, H, W), dtype=bool)\n",
    "\n",
    "    if hasattr(inst, 'scores') and inst.scores is not None:\n",
    "        scores = inst.scores.detach().cpu().numpy()\n",
    "    if hasattr(inst, 'labels') and inst.labels is not None:\n",
    "        labels = inst.labels.detach().cpu().numpy()\n",
    "\n",
    "print(f'Unique semantic classes: {np.unique(sem_map).tolist()}')\n",
    "print(f'Predicted instances: {len(masks)}')\n",
    "if len(scores) > 0:\n",
    "    print(f'Score range: {float(scores.min()):.4f} .. {float(scores.max()):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palette: background, flat_roof, sloped_roof\n",
    "palette = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 255, 0],\n",
    "    [255, 0, 0],\n",
    "], dtype=np.uint8)\n",
    "\n",
    "if hasattr(model, 'dataset_meta') and isinstance(model.dataset_meta, dict):\n",
    "    model_palette = model.dataset_meta.get('palette', None)\n",
    "    if model_palette is not None and len(model_palette) >= 3:\n",
    "        palette = np.array(model_palette, dtype=np.uint8)\n",
    "\n",
    "sem_vis = palette[np.clip(sem_map, 0, len(palette) - 1)]\n",
    "overlay = cv2.addWeighted(img_rgb, 0.60, sem_vis, 0.40, 0.0)\n",
    "\n",
    "MIN_SCORE = 0.25\n",
    "for i, mask in enumerate(masks):\n",
    "    if i < len(scores) and float(scores[i]) < MIN_SCORE:\n",
    "        continue\n",
    "\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(overlay, contours, -1, (255, 255, 255), 1)\n",
    "\n",
    "    if len(contours) > 0 and i < len(scores):\n",
    "        c = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        txt = f'{float(scores[i]):.2f}'\n",
    "        if i < len(labels):\n",
    "            txt = f'cls:{int(labels[i])} {txt}'\n",
    "        cv2.putText(\n",
    "            overlay,\n",
    "            txt,\n",
    "            (x, max(0, y - 5)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.4,\n",
    "            (255, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "cv2.imwrite(str(OVERLAY_PATH), cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "cv2.imwrite(str(SEM_MASK_PATH), sem_map)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_rgb)\n",
    "plt.title('Input')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(overlay)\n",
    "plt.title('Segmentation Overlay')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f'Saved overlay: {OVERLAY_PATH}')\n",
    "print(f'Saved semantic mask: {SEM_MASK_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccece88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'project_root': str(PROJECT_ROOT),\n",
    "    'input_image': str(INPUT_IMAGE_PATH),\n",
    "    'config': str(CONFIG_PATH),\n",
    "    'checkpoint': str(CHECKPOINT_PATH),\n",
    "    'device': DEVICE,\n",
    "    'image_size': [int(H), int(W)],\n",
    "    'semantic_classes': [int(x) for x in np.unique(sem_map)],\n",
    "    'instance_count': int(len(masks)),\n",
    "    'score_mean': float(scores.mean()) if len(scores) > 0 else None,\n",
    "    'score_max': float(scores.max()) if len(scores) > 0 else None,\n",
    "    'overlay_path': str(OVERLAY_PATH),\n",
    "    'semantic_mask_path': str(SEM_MASK_PATH),\n",
    "}\n",
    "\n",
    "with SUMMARY_PATH.open('w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(json.dumps(summary, indent=2, ensure_ascii=False))\n",
    "print(f'Saved summary: {SUMMARY_PATH}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
