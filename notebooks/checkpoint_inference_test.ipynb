{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6980e0e6",
   "metadata": {},
   "source": [
    "# DeepRoof Checkpoint Inference (PNG/JPG)\n",
    "\n",
    "Notebook validates the checkpoint, loads the DeepRoof model, runs segmentation on one image, and saves visual results.\n",
    "\n",
    "**Preprocessing:** Any input image is automatically center-cropped to a square and resized to 512×512 to match the training resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== CONFIGURATION ========================\n",
    "# Training resolution — model was trained on 512×512 OmniCity crops.\n",
    "MODEL_INPUT_SIZE = 512\n",
    "\n",
    "\n",
    "def detect_project_root() -> Path:\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path('/workspace/roof'),\n",
    "        Path('/Users/voskan/Desktop/DeepRoof-2026'),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if (c / 'configs').exists() and (c / 'deeproof').exists():\n",
    "            return c\n",
    "    raise FileNotFoundError('Could not auto-detect project root with configs/ and deeproof/.')\n",
    "\n",
    "\n",
    "def resolve_checkpoint(work_dir: Path, fallback_ckpt: Path) -> Path:\n",
    "    \"\"\"Prefer last_checkpoint pointer to avoid accidentally using stale weights.\"\"\"\n",
    "    last_ckpt_ptr = work_dir / 'last_checkpoint'\n",
    "    if last_ckpt_ptr.exists():\n",
    "        target = last_ckpt_ptr.read_text(encoding='utf-8').strip()\n",
    "        if target:\n",
    "            t = Path(target)\n",
    "            if not t.is_absolute():\n",
    "                t = work_dir / t\n",
    "            if t.exists():\n",
    "                return t\n",
    "\n",
    "    if fallback_ckpt.exists():\n",
    "        return fallback_ckpt\n",
    "\n",
    "    return fallback_ckpt\n",
    "\n",
    "\n",
    "PROJECT_ROOT = detect_project_root()\n",
    "\n",
    "# Use production config — it contains test_pipeline for inference.\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'deeproof_production_swin_L.py'\n",
    "\n",
    "# Set training run directory here. Notebook will auto-pick `last_checkpoint` if present.\n",
    "WORK_DIR = PROJECT_ROOT / 'work_dirs' / 'swin_l_scratch_v1'\n",
    "SERVER_CHECKPOINT_PATH = WORK_DIR / 'iter_8000.pth'\n",
    "LOCAL_ANALYSIS_CHECKPOINT_PATH = Path('/Users/voskan/Downloads/iter_8000.pth')\n",
    "CHECKPOINT_PATH = resolve_checkpoint(WORK_DIR, SERVER_CHECKPOINT_PATH)\n",
    "if not CHECKPOINT_PATH.exists() and LOCAL_ANALYSIS_CHECKPOINT_PATH.exists():\n",
    "    CHECKPOINT_PATH = LOCAL_ANALYSIS_CHECKPOINT_PATH\n",
    "\n",
    "# Set your test image here (PNG/JPG/TIF).\n",
    "INPUT_IMAGE_PATH = Path('/workspace/test.png')\n",
    "if not INPUT_IMAGE_PATH.exists():\n",
    "    fallback = PROJECT_ROOT / 'test.png'\n",
    "    if fallback.exists():\n",
    "        INPUT_IMAGE_PATH = fallback\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs' / 'checkpoint_inference'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OVERLAY_PATH = OUTPUT_DIR / 'test_segmentation_overlay.png'\n",
    "SEM_MASK_PATH = OUTPUT_DIR / 'test_semantic_mask.png'\n",
    "SUMMARY_PATH = OUTPUT_DIR / 'test_inference_summary.json'\n",
    "POLYGONS_JSON_PATH = OUTPUT_DIR / 'test_roof_polygons.json'\n",
    "POLYGONS_GEOJSON_PATH = OUTPUT_DIR / 'test_roof_polygons.geojson'\n",
    "PREPROCESSED_PATH = OUTPUT_DIR / 'test_preprocessed.png'\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'PROJECT_ROOT: {PROJECT_ROOT}')\n",
    "print(f'CONFIG: {CONFIG_PATH}')\n",
    "print(f'WORK_DIR: {WORK_DIR}')\n",
    "print(f'CHECKPOINT: {CHECKPOINT_PATH}')\n",
    "if CHECKPOINT_PATH.exists():\n",
    "    print(f'CHECKPOINT mtime: {CHECKPOINT_PATH.stat().st_mtime}')\n",
    "print(f'INPUT: {INPUT_IMAGE_PATH}')\n",
    "print(f'MODEL_INPUT_SIZE: {MODEL_INPUT_SIZE}x{MODEL_INPUT_SIZE}')\n",
    "print(f'DEVICE: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMAGE PREPROCESSING ========================\n",
    "# Center-crop to square, then resize to MODEL_INPUT_SIZE.\n",
    "# This ensures ANY input image matches what the model was trained on.\n",
    "\n",
    "for p in (CONFIG_PATH, CHECKPOINT_PATH, INPUT_IMAGE_PATH):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f'Path not found: {p}')\n",
    "\n",
    "img_orig_bgr = cv2.imread(str(INPUT_IMAGE_PATH), cv2.IMREAD_COLOR)\n",
    "if img_orig_bgr is None:\n",
    "    raise RuntimeError(f'Could not load image: {INPUT_IMAGE_PATH}')\n",
    "\n",
    "orig_h, orig_w = img_orig_bgr.shape[:2]\n",
    "print(f'Original image size: {orig_w}x{orig_h}')\n",
    "\n",
    "# Step 1: Center-crop to the largest square\n",
    "crop_size = min(orig_h, orig_w)\n",
    "y_start = (orig_h - crop_size) // 2\n",
    "x_start = (orig_w - crop_size) // 2\n",
    "img_cropped = img_orig_bgr[y_start:y_start+crop_size, x_start:x_start+crop_size]\n",
    "print(f'Center-cropped to: {crop_size}x{crop_size}')\n",
    "\n",
    "# Step 2: Resize to model input size\n",
    "img_bgr = cv2.resize(img_cropped, (MODEL_INPUT_SIZE, MODEL_INPUT_SIZE), interpolation=cv2.INTER_AREA)\n",
    "print(f'Resized to: {MODEL_INPUT_SIZE}x{MODEL_INPUT_SIZE}')\n",
    "\n",
    "# Save preprocessed image — inference_model will load from file via test_pipeline\n",
    "cv2.imwrite(str(PREPROCESSED_PATH), img_bgr)\n",
    "print(f'Preprocessed image saved: {PREPROCESSED_PATH}')\n",
    "\n",
    "# For visualization later\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "H, W = img_rgb.shape[:2]\n",
    "\n",
    "# Also keep original for side-by-side\n",
    "img_orig_rgb = cv2.cvtColor(img_orig_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_orig_rgb)\n",
    "plt.title(f'Original ({orig_w}x{orig_h})')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(f'Preprocessed ({MODEL_INPUT_SIZE}x{MODEL_INPUT_SIZE})')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint compatibility inspection\n",
    "os.environ.setdefault('TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD', '1')\n",
    "\n",
    "def inspect_checkpoint(path: Path):\n",
    "    try:\n",
    "        ckpt = torch.load(str(path), map_location='cpu', weights_only=False)\n",
    "    except TypeError:\n",
    "        ckpt = torch.load(str(path), map_location='cpu')\n",
    "\n",
    "    if not isinstance(ckpt, dict):\n",
    "        return {'type': str(type(ckpt)), 'error': 'checkpoint is not a dict'}\n",
    "\n",
    "    state_dict = ckpt.get('state_dict', ckpt.get('model', None))\n",
    "    info = {\n",
    "        'top_keys': list(ckpt.keys()),\n",
    "        'has_state_dict': isinstance(state_dict, dict),\n",
    "        'meta_keys': list(ckpt.get('meta', {}).keys()) if isinstance(ckpt.get('meta', None), dict) else [],\n",
    "    }\n",
    "\n",
    "    if isinstance(state_dict, dict):\n",
    "        keys = list(state_dict.keys())\n",
    "        info['num_params'] = len(keys)\n",
    "        info['first_keys'] = keys[:15]\n",
    "        probes = [\n",
    "            'backbone.patch_embed.projection.weight',\n",
    "            'decode_head.query_embed.weight',\n",
    "            'geometry_head.layers.0.weight',\n",
    "            'module.backbone.patch_embed.projection.weight',\n",
    "        ]\n",
    "        info['probe_hits'] = {k: (k in state_dict) for k in probes}\n",
    "\n",
    "    return info\n",
    "\n",
    "ckpt_info = inspect_checkpoint(CHECKPOINT_PATH)\n",
    "print(json.dumps(ckpt_info, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe0695",
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from mmseg.utils import register_all_modules\n",
    "from mmseg.apis import init_model, inference_model\n",
    "\n",
    "register_all_modules(init_default_scope=False)\n",
    "\n",
    "# Ensure custom modules are imported and registered.\n",
    "import deeproof.models.backbones.swin_v2_compat\n",
    "import deeproof.models.deeproof_model\n",
    "import deeproof.models.heads.mask2former_head\n",
    "import deeproof.models.heads.geometry_head\n",
    "import deeproof.models.losses\n",
    "\n",
    "model = init_model(str(CONFIG_PATH), str(CHECKPOINT_PATH), device=DEVICE)\n",
    "\n",
    "# Use the test_pipeline from the config.\n",
    "# Production config defines: LoadImageFromFile -> Resize(512,512) -> PackSegInputs\n",
    "if hasattr(model.cfg, 'test_pipeline') and model.cfg.test_pipeline:\n",
    "    print('Using test_pipeline from config:', model.cfg.test_pipeline)\n",
    "else:\n",
    "    # Fallback if test_pipeline is missing from config\n",
    "    model.cfg.test_pipeline = [\n",
    "        dict(type='LoadImageFromFile'),\n",
    "        dict(type='Resize', scale=(MODEL_INPUT_SIZE, MODEL_INPUT_SIZE), keep_ratio=False),\n",
    "        dict(type='PackSegInputs'),\n",
    "    ]\n",
    "    print('Using fallback test_pipeline:', model.cfg.test_pipeline)\n",
    "\n",
    "# Use 'whole' mode for Mask2Former (slide mode adds stitching artifacts).\n",
    "from mmengine.config import ConfigDict\n",
    "model.test_cfg = ConfigDict(dict(mode='whole'))\n",
    "\n",
    "model.eval()\n",
    "print('Model loaded successfully.')\n",
    "print('model.test_cfg:', model.test_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== INFERENCE ========================\n",
    "# Run on the PREPROCESSED image (already center-cropped and resized to 512×512)\n",
    "result = inference_model(model, str(PREPROCESSED_PATH))\n",
    "if isinstance(result, (list, tuple)):\n",
    "    result = result[0]\n",
    "\n",
    "# Semantic map\n",
    "if hasattr(result, 'pred_sem_seg') and hasattr(result.pred_sem_seg, 'data'):\n",
    "    sem_map = result.pred_sem_seg.data.squeeze(0).detach().cpu().numpy().astype(np.uint8)\n",
    "else:\n",
    "    sem_map = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "if sem_map.shape != (H, W):\n",
    "    sem_map = cv2.resize(sem_map, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Instances (if available)\n",
    "masks = np.zeros((0, H, W), dtype=bool)\n",
    "scores = np.array([], dtype=np.float32)\n",
    "labels = np.array([], dtype=np.int64)\n",
    "instance_source = 'none'\n",
    "\n",
    "if hasattr(result, 'pred_instances') and result.pred_instances is not None:\n",
    "    inst = result.pred_instances\n",
    "    if hasattr(inst, 'masks') and inst.masks is not None:\n",
    "        masks_t = inst.masks\n",
    "        if torch.is_tensor(masks_t):\n",
    "            masks_np = masks_t.detach().cpu().numpy().astype(bool)\n",
    "            if masks_np.ndim == 3:\n",
    "                resized_masks = []\n",
    "                for m in masks_np:\n",
    "                    if m.shape != (H, W):\n",
    "                        m = cv2.resize(m.astype(np.uint8), (W, H), interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "                    resized_masks.append(m)\n",
    "                masks = np.stack(resized_masks, axis=0) if resized_masks else np.zeros((0, H, W), dtype=bool)\n",
    "\n",
    "    if hasattr(inst, 'scores') and inst.scores is not None:\n",
    "        scores = inst.scores.detach().cpu().numpy()\n",
    "    if hasattr(inst, 'labels') and inst.labels is not None:\n",
    "        labels = inst.labels.detach().cpu().numpy()\n",
    "\n",
    "    if len(masks) > 0:\n",
    "        instance_source = 'model_pred_instances'\n",
    "\n",
    "# Fallback: build pseudo-instances from semantic connected components\n",
    "if len(masks) == 0:\n",
    "    comp_masks = []\n",
    "    comp_labels = []\n",
    "    min_area = 64\n",
    "    for cls_id in np.unique(sem_map):\n",
    "        cls_id = int(cls_id)\n",
    "        if cls_id <= 0 or cls_id == 255:\n",
    "            continue\n",
    "        binary = (sem_map == cls_id).astype(np.uint8)\n",
    "        num_comp, comp = cv2.connectedComponents(binary, connectivity=8)\n",
    "        for comp_id in range(1, int(num_comp)):\n",
    "            m = (comp == comp_id)\n",
    "            if int(m.sum()) < min_area:\n",
    "                continue\n",
    "            comp_masks.append(m)\n",
    "            comp_labels.append(cls_id)\n",
    "\n",
    "    if comp_masks:\n",
    "        masks = np.stack(comp_masks, axis=0).astype(bool)\n",
    "        labels = np.array(comp_labels, dtype=np.int64)\n",
    "        scores = np.array([], dtype=np.float32)\n",
    "        instance_source = 'semantic_connected_components_fallback'\n",
    "\n",
    "# ======================== DIAGNOSTICS ========================\n",
    "unique_classes = np.unique(sem_map).tolist()\n",
    "class_areas = {int(c): float((sem_map == c).sum()) / float(sem_map.size) for c in unique_classes}\n",
    "roof_ratio = float((sem_map > 0).sum()) / float(sem_map.size)\n",
    "print(f'Unique semantic classes: {unique_classes}')\n",
    "print(f'Class area ratios: {class_areas}')\n",
    "print(f'Roof pixel ratio: {roof_ratio:.4f}')\n",
    "print(f'Predicted instances: {len(masks)} (source={instance_source})')\n",
    "if len(scores) > 0:\n",
    "    print(f'Score range: {float(scores.min()):.4f} .. {float(scores.max()):.4f}')\n",
    "if roof_ratio > 0.90:\n",
    "    print('WARNING: segmentation marks >90% pixels as roof.')\n",
    "    print('  Possible causes: model still undertrained, or test image too different from training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== RAW LOGITS DIAGNOSTICS ========================\n",
    "# Check what the model actually produces BEFORE argmax\n",
    "# This helps distinguish 'model not converged' from 'inference bug'\n",
    "\n",
    "# Run a manual forward pass to inspect raw outputs\n",
    "from mmengine.dataset import Compose\n",
    "from copy import deepcopy\n",
    "\n",
    "pipeline = Compose(model.cfg.test_pipeline)\n",
    "data = dict(img_path=str(PREPROCESSED_PATH))\n",
    "data = pipeline(data)\n",
    "\n",
    "# Prepare input for model\n",
    "data_batch = dict(\n",
    "    inputs=[data['inputs']],\n",
    "    data_samples=[data['data_samples']],\n",
    ")\n",
    "# Preprocess (normalize, pad)\n",
    "data_batch = model.data_preprocessor(data_batch, False)\n",
    "inputs = data_batch['inputs']\n",
    "\n",
    "# Forward through backbone + decode_head\n",
    "with torch.no_grad():\n",
    "    x = model.extract_feat(inputs)\n",
    "    all_cls_scores, all_mask_preds = model.decode_head(x, data_batch['data_samples'])\n",
    "\n",
    "# Analyze last decoder layer outputs\n",
    "cls_scores = all_cls_scores[-1]  # [B, num_queries, num_classes+1]\n",
    "mask_preds = all_mask_preds[-1]  # [B, num_queries, H, W]\n",
    "\n",
    "print(f'cls_scores shape: {cls_scores.shape}')\n",
    "print(f'mask_preds shape: {mask_preds.shape}')\n",
    "\n",
    "# Per-query class predictions\n",
    "cls_probs = torch.softmax(cls_scores[0], dim=-1)  # [Q, C+1]\n",
    "pred_classes = cls_probs[:, :-1].argmax(dim=-1)    # ignore no-obj column\n",
    "no_obj_probs = cls_probs[:, -1]                     # P(no-object)\n",
    "max_cls_probs = cls_probs[:, :-1].max(dim=-1).values\n",
    "\n",
    "print(f'\\n--- Per-query analysis (top 20 by confidence) ---')\n",
    "sorted_idx = max_cls_probs.argsort(descending=True)\n",
    "for rank, qi in enumerate(sorted_idx[:20]):\n",
    "    qi = int(qi)\n",
    "    mask_vals = mask_preds[0, qi].sigmoid()\n",
    "    mask_coverage = float((mask_vals > 0.5).float().mean())\n",
    "    print(f'  Query {qi:3d}: class={int(pred_classes[qi])}, '\n",
    "          f'P(class)={float(max_cls_probs[qi]):.3f}, '\n",
    "          f'P(no-obj)={float(no_obj_probs[qi]):.3f}, '\n",
    "          f'mask_coverage={mask_coverage:.3f}')\n",
    "\n",
    "# Overall statistics\n",
    "mask_sigmoid = mask_preds[0].sigmoid()  # [Q, H, W]\n",
    "print(f'\\n--- Mask statistics ---')\n",
    "print(f'mask sigmoid range: {float(mask_sigmoid.min()):.4f} .. {float(mask_sigmoid.max()):.4f}')\n",
    "print(f'mask sigmoid mean: {float(mask_sigmoid.mean()):.4f}')\n",
    "print(f'Queries with >50% coverage: {int((mask_sigmoid.mean(dim=(-1,-2)) > 0.5).sum())} / {mask_sigmoid.shape[0]}')\n",
    "print(f'Queries with <10% coverage: {int((mask_sigmoid.mean(dim=(-1,-2)) < 0.1).sum())} / {mask_sigmoid.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== VISUALIZATION ========================\n",
    "# Palette: background, flat_roof, sloped_roof\n",
    "palette = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 255, 0],\n",
    "    [255, 0, 0],\n",
    "], dtype=np.uint8)\n",
    "\n",
    "if hasattr(model, 'dataset_meta') and isinstance(model.dataset_meta, dict):\n",
    "    model_palette = model.dataset_meta.get('palette', None)\n",
    "    if model_palette is not None and len(model_palette) >= 3:\n",
    "        palette = np.array(model_palette, dtype=np.uint8)\n",
    "\n",
    "sem_vis = palette[np.clip(sem_map, 0, len(palette) - 1)]\n",
    "overlay = cv2.addWeighted(img_rgb, 0.60, sem_vis, 0.40, 0.0)\n",
    "\n",
    "# --- Polygon extraction from instance masks ---\n",
    "roof_polygons = []\n",
    "MIN_SCORE = 0.25\n",
    "MIN_POLY_AREA = 64\n",
    "\n",
    "for i, mask in enumerate(masks):\n",
    "    if i < len(scores) and float(scores[i]) < MIN_SCORE:\n",
    "        continue\n",
    "\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        area = float(cv2.contourArea(contour))\n",
    "        if area < MIN_POLY_AREA:\n",
    "            continue\n",
    "\n",
    "        epsilon = 0.003 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        poly = approx.reshape(-1, 2)\n",
    "        if poly.shape[0] < 3:\n",
    "            continue\n",
    "\n",
    "        cls_id = int(labels[i]) if i < len(labels) else 1\n",
    "        score = float(scores[i]) if i < len(scores) else None\n",
    "\n",
    "        roof_polygons.append({\n",
    "            'polygon_id': len(roof_polygons),\n",
    "            'class_id': cls_id,\n",
    "            'score': score,\n",
    "            'area_px': area,\n",
    "            'points_xy': poly.astype(int).tolist(),\n",
    "        })\n",
    "\n",
    "# Draw polygons\n",
    "for poly_obj in roof_polygons:\n",
    "    pts = np.array(poly_obj['points_xy'], dtype=np.int32).reshape(-1, 1, 2)\n",
    "    cv2.polylines(overlay, [pts], True, (255, 255, 255), 2)\n",
    "\n",
    "    # label near polygon centroid\n",
    "    m = cv2.moments(pts)\n",
    "    if m['m00'] > 0:\n",
    "        cx = int(m['m10'] / m['m00'])\n",
    "        cy = int(m['m01'] / m['m00'])\n",
    "    else:\n",
    "        cx, cy = pts[0, 0, 0], pts[0, 0, 1]\n",
    "\n",
    "    txt = f\"id:{poly_obj['polygon_id']} cls:{poly_obj['class_id']}\"\n",
    "    if poly_obj['score'] is not None:\n",
    "        txt += f\" {poly_obj['score']:.2f}\"\n",
    "    cv2.putText(overlay, txt, (cx, max(0, cy - 4)), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "# Save masks/overlay\n",
    "cv2.imwrite(str(OVERLAY_PATH), cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "cv2.imwrite(str(SEM_MASK_PATH), sem_map)\n",
    "\n",
    "# Save polygons JSON\n",
    "with POLYGONS_JSON_PATH.open('w', encoding='utf-8') as f:\n",
    "    json.dump({'image': str(INPUT_IMAGE_PATH), 'polygons': roof_polygons}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save simple GeoJSON-like pixel-space output\n",
    "geojson = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': []\n",
    "}\n",
    "for poly_obj in roof_polygons:\n",
    "    coords = [[float(x), float(y)] for x, y in poly_obj['points_xy']]\n",
    "    # close ring\n",
    "    if coords and coords[0] != coords[-1]:\n",
    "        coords.append(coords[0])\n",
    "    geojson['features'].append({\n",
    "        'type': 'Feature',\n",
    "        'properties': {\n",
    "            'polygon_id': poly_obj['polygon_id'],\n",
    "            'class_id': poly_obj['class_id'],\n",
    "            'score': poly_obj['score'],\n",
    "            'area_px': poly_obj['area_px'],\n",
    "        },\n",
    "        'geometry': {\n",
    "            'type': 'Polygon',\n",
    "            'coordinates': [coords],\n",
    "        },\n",
    "    })\n",
    "\n",
    "with POLYGONS_GEOJSON_PATH.open('w', encoding='utf-8') as f:\n",
    "    json.dump(geojson, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_rgb)\n",
    "plt.title('Input (preprocessed)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(sem_vis)\n",
    "plt.title('Semantic map')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(overlay)\n",
    "plt.title(f'Roof polygons: {len(roof_polygons)}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f'Saved overlay: {OVERLAY_PATH}')\n",
    "print(f'Saved semantic mask: {SEM_MASK_PATH}')\n",
    "print(f'Saved polygons JSON: {POLYGONS_JSON_PATH}')\n",
    "print(f'Saved polygons GeoJSON: {POLYGONS_GEOJSON_PATH}')\n",
    "print(f'Extracted polygons: {len(roof_polygons)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccece88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'project_root': str(PROJECT_ROOT),\n",
    "    'work_dir': str(WORK_DIR),\n",
    "    'input_image': str(INPUT_IMAGE_PATH),\n",
    "    'preprocessed_image': str(PREPROCESSED_PATH),\n",
    "    'config': str(CONFIG_PATH),\n",
    "    'checkpoint': str(CHECKPOINT_PATH),\n",
    "    'device': DEVICE,\n",
    "    'original_size': [int(orig_h), int(orig_w)],\n",
    "    'model_input_size': [MODEL_INPUT_SIZE, MODEL_INPUT_SIZE],\n",
    "    'semantic_classes': [int(x) for x in np.unique(sem_map)],\n",
    "    'class_areas': class_areas,\n",
    "    'roof_pixel_ratio': float((sem_map > 0).sum()) / float(sem_map.size),\n",
    "    'instance_count': int(len(masks)),\n",
    "    'instance_source': instance_source,\n",
    "    'polygon_count': int(len(roof_polygons)),\n",
    "    'score_mean': float(scores.mean()) if len(scores) > 0 else None,\n",
    "    'score_max': float(scores.max()) if len(scores) > 0 else None,\n",
    "    'overlay_path': str(OVERLAY_PATH),\n",
    "    'semantic_mask_path': str(SEM_MASK_PATH),\n",
    "    'polygons_json_path': str(POLYGONS_JSON_PATH),\n",
    "    'polygons_geojson_path': str(POLYGONS_GEOJSON_PATH),\n",
    "}\n",
    "\n",
    "with SUMMARY_PATH.open('w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(json.dumps(summary, indent=2, ensure_ascii=False))\n",
    "print(f'Saved summary: {SUMMARY_PATH}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}