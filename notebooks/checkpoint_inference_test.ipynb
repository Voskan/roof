{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6980e0e6",
   "metadata": {},
   "source": [
    "# DeepRoof Checkpoint Inference (PNG/JPG)\n",
    "\n",
    "Runs segmentation on any satellite image using a trained DeepRoof checkpoint.\n",
    "\n",
    "**Preprocessing:** Center-crop to square → resize to 512×512 (training resolution).\n",
    "\n",
    "**Post-processing:** Uses panoptic-style per-pixel query assignment with mask thresholding\n",
    "instead of the default MMSeg einsum aggregation (which fails when queries are class-biased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== CONFIGURATION ========================\n",
    "MODEL_INPUT_SIZE = 512  # Training resolution (native OmniCity size)\n",
    "MASK_THRESHOLD = 0.5    # Min sigmoid confidence for a pixel to be non-background\n",
    "\n",
    "\n",
    "def detect_project_root() -> Path:\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path('/workspace/roof'),\n",
    "        Path('/Users/voskan/Desktop/DeepRoof-2026'),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if (c / 'configs').exists() and (c / 'deeproof').exists():\n",
    "            return c\n",
    "    raise FileNotFoundError('Could not auto-detect project root.')\n",
    "\n",
    "\n",
    "def resolve_checkpoint(work_dir: Path, fallback_ckpt: Path) -> Path:\n",
    "    last_ckpt_ptr = work_dir / 'last_checkpoint'\n",
    "    if last_ckpt_ptr.exists():\n",
    "        target = last_ckpt_ptr.read_text(encoding='utf-8').strip()\n",
    "        if target:\n",
    "            t = Path(target)\n",
    "            if not t.is_absolute():\n",
    "                t = work_dir / t\n",
    "            if t.exists():\n",
    "                return t\n",
    "    return fallback_ckpt\n",
    "\n",
    "\n",
    "PROJECT_ROOT = detect_project_root()\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'deeproof_production_swin_L.py'\n",
    "\n",
    "WORK_DIR = PROJECT_ROOT / 'work_dirs' / 'swin_l_scratch_v1'\n",
    "SERVER_CHECKPOINT_PATH = WORK_DIR / 'iter_16000.pth'\n",
    "LOCAL_ANALYSIS_CHECKPOINT_PATH = Path('/Users/voskan/Downloads/iter_16000.pth')\n",
    "CHECKPOINT_PATH = resolve_checkpoint(WORK_DIR, SERVER_CHECKPOINT_PATH)\n",
    "if not CHECKPOINT_PATH.exists() and LOCAL_ANALYSIS_CHECKPOINT_PATH.exists():\n",
    "    CHECKPOINT_PATH = LOCAL_ANALYSIS_CHECKPOINT_PATH\n",
    "\n",
    "# ---- TEST IMAGE ----\n",
    "# Option A: External image (any size — will be center-cropped and resized)\n",
    "INPUT_IMAGE_PATH = Path('/workspace/test.png')\n",
    "# Option B: OmniCity validation image (uncomment to test on training distribution)\n",
    "# INPUT_IMAGE_PATH = Path('data/OmniCity/images/FIRST_VAL_IMAGE.jpg')\n",
    "if not INPUT_IMAGE_PATH.exists():\n",
    "    fallback = PROJECT_ROOT / 'test.png'\n",
    "    if fallback.exists():\n",
    "        INPUT_IMAGE_PATH = fallback\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs' / 'checkpoint_inference'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OVERLAY_PATH = OUTPUT_DIR / 'test_segmentation_overlay.png'\n",
    "SEM_MASK_PATH = OUTPUT_DIR / 'test_semantic_mask.png'\n",
    "SUMMARY_PATH = OUTPUT_DIR / 'test_inference_summary.json'\n",
    "POLYGONS_JSON_PATH = OUTPUT_DIR / 'test_roof_polygons.json'\n",
    "POLYGONS_GEOJSON_PATH = OUTPUT_DIR / 'test_roof_polygons.geojson'\n",
    "PREPROCESSED_PATH = OUTPUT_DIR / 'test_preprocessed.png'\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'PROJECT_ROOT: {PROJECT_ROOT}')\n",
    "print(f'CONFIG: {CONFIG_PATH}')\n",
    "print(f'CHECKPOINT: {CHECKPOINT_PATH}')\n",
    "print(f'INPUT: {INPUT_IMAGE_PATH}')\n",
    "print(f'MODEL_INPUT_SIZE: {MODEL_INPUT_SIZE}x{MODEL_INPUT_SIZE}')\n",
    "print(f'MASK_THRESHOLD: {MASK_THRESHOLD}')\n",
    "print(f'DEVICE: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== IMAGE PREPROCESSING ========================\n",
    "for p in (CONFIG_PATH, CHECKPOINT_PATH, INPUT_IMAGE_PATH):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f'Path not found: {p}')\n",
    "\n",
    "img_orig_bgr = cv2.imread(str(INPUT_IMAGE_PATH), cv2.IMREAD_COLOR)\n",
    "if img_orig_bgr is None:\n",
    "    raise RuntimeError(f'Could not load image: {INPUT_IMAGE_PATH}')\n",
    "\n",
    "orig_h, orig_w = img_orig_bgr.shape[:2]\n",
    "print(f'Original image size: {orig_w}x{orig_h}')\n",
    "\n",
    "# Center-crop to square\n",
    "crop_size = min(orig_h, orig_w)\n",
    "y_start = (orig_h - crop_size) // 2\n",
    "x_start = (orig_w - crop_size) // 2\n",
    "img_cropped = img_orig_bgr[y_start:y_start+crop_size, x_start:x_start+crop_size]\n",
    "\n",
    "# Resize to model input\n",
    "img_bgr = cv2.resize(img_cropped, (MODEL_INPUT_SIZE, MODEL_INPUT_SIZE), interpolation=cv2.INTER_AREA)\n",
    "cv2.imwrite(str(PREPROCESSED_PATH), img_bgr)\n",
    "print(f'Preprocessed: {orig_w}x{orig_h} -> center crop {crop_size}x{crop_size} -> resize {MODEL_INPUT_SIZE}x{MODEL_INPUT_SIZE}')\n",
    "\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "H, W = img_rgb.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== LOAD MODEL ========================\n",
    "os.environ.setdefault('TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD', '1')\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from mmseg.utils import register_all_modules\n",
    "from mmseg.apis import init_model\n",
    "from mmengine.config import ConfigDict\n",
    "from mmengine.dataset import Compose\n",
    "\n",
    "register_all_modules(init_default_scope=False)\n",
    "\n",
    "import deeproof.models.backbones.swin_v2_compat\n",
    "import deeproof.models.deeproof_model\n",
    "import deeproof.models.heads.mask2former_head\n",
    "import deeproof.models.heads.geometry_head\n",
    "import deeproof.models.losses\n",
    "\n",
    "model = init_model(str(CONFIG_PATH), str(CHECKPOINT_PATH), device=DEVICE)\n",
    "model.test_cfg = ConfigDict(dict(mode='whole'))\n",
    "model.eval()\n",
    "print('Model loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== RAW FORWARD PASS ========================\n",
    "# Instead of using inference_model (which uses einsum aggregation),\n",
    "# we run the model manually and build the semantic map ourselves\n",
    "# using panoptic-style per-pixel query assignment.\n",
    "\n",
    "# Build test pipeline and preprocess\n",
    "test_pipeline = model.cfg.get('test_pipeline', [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='Resize', scale=(MODEL_INPUT_SIZE, MODEL_INPUT_SIZE), keep_ratio=False),\n",
    "    dict(type='PackSegInputs'),\n",
    "])\n",
    "pipeline = Compose(test_pipeline)\n",
    "data = pipeline(dict(img_path=str(PREPROCESSED_PATH)))\n",
    "\n",
    "# Run through data preprocessor (normalize, pad)\n",
    "data_batch = dict(\n",
    "    inputs=[data['inputs']],\n",
    "    data_samples=[data['data_samples']],\n",
    ")\n",
    "data_batch = model.data_preprocessor(data_batch, False)\n",
    "inputs = data_batch['inputs']\n",
    "\n",
    "# Forward: backbone + decode_head\n",
    "with torch.no_grad():\n",
    "    x = model.extract_feat(inputs)\n",
    "    all_cls_scores, all_mask_preds = model.decode_head(x, data_batch['data_samples'])\n",
    "\n",
    "# Last decoder layer outputs (most refined)\n",
    "cls_scores = all_cls_scores[-1][0]   # [Q, C+1]  (100 queries, 4 classes incl. no-obj)\n",
    "mask_preds = all_mask_preds[-1][0]   # [Q, h, w]  (low-res mask logits, e.g. 128x128)\n",
    "\n",
    "num_queries = cls_scores.shape[0]\n",
    "num_classes = cls_scores.shape[1] - 1  # exclude no-object\n",
    "\n",
    "print(f'cls_scores: {cls_scores.shape}  (queries={num_queries}, classes={num_classes}+no_obj)')\n",
    "print(f'mask_preds: {mask_preds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== PANOPTIC-STYLE SEMANTIC MAP ========================\n",
    "# Standard Mask2Former does:  sem = einsum('qc,qhw->chw', softmax(cls)[:,:-1], sigmoid(mask)).argmax(0)\n",
    "# This FAILS when all queries predict the same class (common during early training).\n",
    "#\n",
    "# Better approach (from Mask2Former panoptic paper):\n",
    "#   1. For each pixel, find the query with the HIGHEST mask confidence\n",
    "#   2. If that confidence > threshold → assign that query's predicted class\n",
    "#   3. If confidence < threshold → background (class 0)\n",
    "\n",
    "cls_probs = torch.softmax(cls_scores, dim=-1)  # [Q, C+1]\n",
    "obj_probs = cls_probs[:, :-1]                   # [Q, C]  (without no-object)\n",
    "no_obj_probs = cls_probs[:, -1]                  # [Q]\n",
    "\n",
    "mask_sigmoid = mask_preds.sigmoid()              # [Q, h, w]\n",
    "\n",
    "# Per-query predicted class (best object class)\n",
    "query_class = obj_probs.argmax(dim=-1)           # [Q]\n",
    "query_obj_confidence = obj_probs.max(dim=-1).values  # [Q]\n",
    "\n",
    "# Score each pixel: combine class confidence with mask confidence\n",
    "# score[q, h, w] = P(best_class_q) * sigmoid(mask_q[h,w])\n",
    "pixel_scores = query_obj_confidence.unsqueeze(-1).unsqueeze(-1) * mask_sigmoid  # [Q, h, w]\n",
    "\n",
    "# For each pixel, which query has the highest combined score?\n",
    "best_score_per_pixel, best_query_per_pixel = pixel_scores.max(dim=0)  # [h, w]\n",
    "\n",
    "# Assign class from the winning query\n",
    "sem_map_lowres = query_class[best_query_per_pixel]  # [h, w]\n",
    "\n",
    "# Background: pixels where best mask confidence is below threshold\n",
    "best_mask_per_pixel = mask_sigmoid.max(dim=0).values  # [h, w]\n",
    "sem_map_lowres[best_mask_per_pixel < MASK_THRESHOLD] = 0\n",
    "\n",
    "# Upscale to original image resolution\n",
    "sem_map = F.interpolate(\n",
    "    sem_map_lowres.float().unsqueeze(0).unsqueeze(0),\n",
    "    size=(H, W), mode='nearest'\n",
    ")[0, 0].long().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "# ======================== DIAGNOSTICS ========================\n",
    "unique_classes = np.unique(sem_map).tolist()\n",
    "class_areas = {int(c): float((sem_map == c).sum()) / float(sem_map.size) for c in unique_classes}\n",
    "roof_ratio = float((sem_map > 0).sum()) / float(sem_map.size)\n",
    "\n",
    "print(f'\\n=== SEMANTIC MAP (panoptic post-processing, threshold={MASK_THRESHOLD}) ===')\n",
    "print(f'Unique classes: {unique_classes}')\n",
    "print(f'Class area ratios: {class_areas}')\n",
    "print(f'Roof pixel ratio: {roof_ratio:.4f}')\n",
    "\n",
    "# Per-query diagnostics\n",
    "print(f'\\n=== PER-QUERY ANALYSIS (top 20 by confidence) ===')\n",
    "sorted_idx = query_obj_confidence.argsort(descending=True)\n",
    "for rank, qi in enumerate(sorted_idx[:20]):\n",
    "    qi = int(qi)\n",
    "    mask_cov = float((mask_sigmoid[qi] > 0.5).float().mean())\n",
    "    print(f'  Query {qi:3d}: class={int(query_class[qi])}, '\n",
    "          f'P(class)={float(query_obj_confidence[qi]):.3f}, '\n",
    "          f'P(no-obj)={float(no_obj_probs[qi]):.3f}, '\n",
    "          f'mask_coverage={mask_cov:.3f}')\n",
    "\n",
    "# Mask statistics\n",
    "print(f'\\n=== MASK STATISTICS ===')\n",
    "print(f'Sigmoid range: {float(mask_sigmoid.min()):.4f} .. {float(mask_sigmoid.max()):.4f}')\n",
    "print(f'Sigmoid mean: {float(mask_sigmoid.mean()):.4f}')\n",
    "print(f'Queries with >50% coverage: {int((mask_sigmoid.mean(dim=(-1,-2)) > 0.5).sum())} / {num_queries}')\n",
    "print(f'Queries with any coverage (>0.5): {int(((mask_sigmoid > 0.5).any(dim=-1).any(dim=-1)).sum())} / {num_queries}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== BUILD INSTANCES ========================\n",
    "# Extract connected components from the thresholded semantic map\n",
    "masks_list = []\n",
    "labels_list = []\n",
    "scores_list = []\n",
    "MIN_AREA = 64\n",
    "\n",
    "for cls_id in np.unique(sem_map):\n",
    "    cls_id = int(cls_id)\n",
    "    if cls_id <= 0 or cls_id == 255:\n",
    "        continue\n",
    "    binary = (sem_map == cls_id).astype(np.uint8)\n",
    "    num_comp, comp = cv2.connectedComponents(binary, connectivity=8)\n",
    "    for comp_id in range(1, int(num_comp)):\n",
    "        m = (comp == comp_id)\n",
    "        area = int(m.sum())\n",
    "        if area < MIN_AREA:\n",
    "            continue\n",
    "        masks_list.append(m)\n",
    "        labels_list.append(cls_id)\n",
    "        # Score = mean mask sigmoid within this component\n",
    "        m_lowres = cv2.resize(m.astype(np.uint8), (mask_sigmoid.shape[-1], mask_sigmoid.shape[-2]),\n",
    "                              interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "        mean_conf = float(mask_sigmoid.max(dim=0).values[torch.from_numpy(m_lowres)].mean())\n",
    "        scores_list.append(mean_conf)\n",
    "\n",
    "masks = np.stack(masks_list, axis=0) if masks_list else np.zeros((0, H, W), dtype=bool)\n",
    "labels = np.array(labels_list, dtype=np.int64)\n",
    "scores = np.array(scores_list, dtype=np.float32)\n",
    "\n",
    "print(f'Extracted {len(masks)} instances from semantic map')\n",
    "for i in range(min(10, len(masks))):\n",
    "    print(f'  Instance {i}: class={labels[i]}, score={scores[i]:.3f}, area={int(masks[i].sum())}px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== VISUALIZATION ========================\n",
    "palette = np.array([\n",
    "    [0, 0, 0],       # 0: background (black)\n",
    "    [0, 255, 0],     # 1: flat_roof (green)\n",
    "    [255, 0, 0],     # 2: sloped_roof (red)\n",
    "], dtype=np.uint8)\n",
    "\n",
    "sem_vis = palette[np.clip(sem_map, 0, len(palette) - 1)]\n",
    "overlay = cv2.addWeighted(img_rgb, 0.60, sem_vis, 0.40, 0.0)\n",
    "\n",
    "# Polygon extraction\n",
    "roof_polygons = []\n",
    "MIN_SCORE = 0.3\n",
    "MIN_POLY_AREA = 64\n",
    "\n",
    "for i, mask in enumerate(masks):\n",
    "    if i < len(scores) and float(scores[i]) < MIN_SCORE:\n",
    "        continue\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        area = float(cv2.contourArea(contour))\n",
    "        if area < MIN_POLY_AREA:\n",
    "            continue\n",
    "        epsilon = 0.003 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        poly = approx.reshape(-1, 2)\n",
    "        if poly.shape[0] < 3:\n",
    "            continue\n",
    "        cls_id = int(labels[i]) if i < len(labels) else 1\n",
    "        score = float(scores[i]) if i < len(scores) else None\n",
    "        roof_polygons.append({\n",
    "            'polygon_id': len(roof_polygons),\n",
    "            'class_id': cls_id,\n",
    "            'score': score,\n",
    "            'area_px': area,\n",
    "            'points_xy': poly.astype(int).tolist(),\n",
    "        })\n",
    "\n",
    "# Draw polygons\n",
    "for poly_obj in roof_polygons:\n",
    "    pts = np.array(poly_obj['points_xy'], dtype=np.int32).reshape(-1, 1, 2)\n",
    "    color = (0, 255, 0) if poly_obj['class_id'] == 1 else (255, 0, 0)\n",
    "    cv2.polylines(overlay, [pts], True, (255, 255, 255), 2)\n",
    "    m = cv2.moments(pts)\n",
    "    if m['m00'] > 0:\n",
    "        cx, cy = int(m['m10'] / m['m00']), int(m['m01'] / m['m00'])\n",
    "    else:\n",
    "        cx, cy = int(pts[0, 0, 0]), int(pts[0, 0, 1])\n",
    "    txt = f\"cls:{poly_obj['class_id']}\"\n",
    "    if poly_obj['score'] is not None:\n",
    "        txt += f\" {poly_obj['score']:.2f}\"\n",
    "    cv2.putText(overlay, txt, (cx, max(0, cy - 4)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "# Save\n",
    "cv2.imwrite(str(OVERLAY_PATH), cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "cv2.imwrite(str(SEM_MASK_PATH), sem_map)\n",
    "with POLYGONS_JSON_PATH.open('w', encoding='utf-8') as f:\n",
    "    json.dump({'image': str(INPUT_IMAGE_PATH), 'polygons': roof_polygons}, f, indent=2)\n",
    "\n",
    "# GeoJSON\n",
    "geojson = {'type': 'FeatureCollection', 'features': []}\n",
    "for poly_obj in roof_polygons:\n",
    "    coords = [[float(x), float(y)] for x, y in poly_obj['points_xy']]\n",
    "    if coords and coords[0] != coords[-1]:\n",
    "        coords.append(coords[0])\n",
    "    geojson['features'].append({\n",
    "        'type': 'Feature',\n",
    "        'properties': {k: v for k, v in poly_obj.items() if k != 'points_xy'},\n",
    "        'geometry': {'type': 'Polygon', 'coordinates': [coords]},\n",
    "    })\n",
    "with POLYGONS_GEOJSON_PATH.open('w', encoding='utf-8') as f:\n",
    "    json.dump(geojson, f, indent=2)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(f'Input ({W}x{H})')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(sem_vis)\n",
    "plt.title(f'Semantic (classes: {unique_classes})')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(overlay)\n",
    "plt.title(f'Overlay ({len(roof_polygons)} polygons)')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Saved overlay: {OVERLAY_PATH}')\n",
    "print(f'Saved polygons: {POLYGONS_JSON_PATH}')\n",
    "print(f'Total roof polygons: {len(roof_polygons)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== MASK VISUALIZATION ========================\n",
    "# Show top 6 queries' raw masks to verify spatial discrimination\n",
    "\n",
    "top_queries = sorted_idx[:6]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for idx, qi in enumerate(top_queries):\n",
    "    qi = int(qi)\n",
    "    ax = axes[idx // 3][idx % 3]\n",
    "    mask_vis = mask_sigmoid[qi].cpu().numpy()\n",
    "    ax.imshow(mask_vis, cmap='hot', vmin=0, vmax=1)\n",
    "    cov = float((mask_sigmoid[qi] > 0.5).float().mean())\n",
    "    ax.set_title(f'Query {qi}: cls={int(query_class[qi])}, P={float(query_obj_confidence[qi]):.2f}, cov={cov:.3f}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Top 6 query masks (sigmoid)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccece88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== SUMMARY ========================\n",
    "summary = {\n",
    "    'input_image': str(INPUT_IMAGE_PATH),\n",
    "    'checkpoint': str(CHECKPOINT_PATH),\n",
    "    'original_size': [int(orig_h), int(orig_w)],\n",
    "    'model_input_size': MODEL_INPUT_SIZE,\n",
    "    'mask_threshold': MASK_THRESHOLD,\n",
    "    'semantic_classes': unique_classes,\n",
    "    'class_areas': class_areas,\n",
    "    'roof_pixel_ratio': roof_ratio,\n",
    "    'polygon_count': len(roof_polygons),\n",
    "}\n",
    "with SUMMARY_PATH.open('w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}