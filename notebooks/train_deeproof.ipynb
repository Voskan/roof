{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ  DeepRoof-2026: Master Training Lab\n",
    "\n",
    "### ðŸ›  Step 1: System-Level Environment Initialization\n",
    "This cell handles **MMCV Source Compilation** (for Torch 2.4+), **MMSegmentation Repair**, and **CUDA Linking** directly in the system environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "import ctypes\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print('ðŸ›° Initializing DeepRepair Protocol V6 (Comprehensive)....')\n",
    "\n",
    "# --- 1. PATH RESOLUTION (NO VENV) ---\n",
    "project_root = Path('/workspace/roof')\n",
    "if not project_root.exists():\n",
    "    project_root = Path(os.getcwd()).parent\n",
    "\n",
    "# Add project root to sys.path if not present\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    \n",
    "print(f'ðŸ“‚ Project Root: {project_root}')\n",
    "print(f'ðŸ Python: {sys.executable}')\n",
    "\n",
    "# --- 2. NUCLEAR CUDA LINKER (System Scan) ---\n",
    "def nuclear_cuda_fix():\n",
    "    if not torch.cuda.is_available():\n",
    "        print('â„ï¸ CPU Mode: Skipping CUDA linking.')\n",
    "        return True\n",
    "\n",
    "    print('ðŸ” Searching for libcudart.so...')\n",
    "    # Common locations in containers\n",
    "    search_patterns = [\n",
    "        '/usr/local/cuda*/lib64/libcudart.so*',\n",
    "        '/usr/lib/x86_64-linux-gnu/libcudart.so*',\n",
    "        '/usr/lib/libcudart.so*'\n",
    "    ]\n",
    "    \n",
    "    found_lib = None\n",
    "    for pattern in search_patterns:\n",
    "        matches = glob.glob(pattern)\n",
    "        if matches:\n",
    "            # Prefer specific version if multiple\n",
    "            found_lib = sorted(matches)[-1]\n",
    "            break\n",
    "            \n",
    "    if found_lib:\n",
    "        print(f'ðŸ“ Found linking target: {found_lib}')\n",
    "        try:\n",
    "            ctypes.CDLL(found_lib, mode=ctypes.RTLD_GLOBAL)\n",
    "            print('âœ… CUDA Runtime force-loaded.')\n",
    "        except Exception as e:\n",
    "            print(f'âš ï¸ Force-load warning: {e}')\n",
    "    else:\n",
    "        print('âš ï¸ Could not find libcudart.so in standard paths. Assuming built-in.')\n",
    "    return True\n",
    "\n",
    "# --- 3. MMCV SOURCE COMPILER (Torch 2.4+) ---\n",
    "def match_mmcv_to_torch():\n",
    "    torch_ver = torch.__version__\n",
    "    print(f'ðŸ” Detected: Torch {torch_ver}')\n",
    "    \n",
    "    mmcv_ok = False\n",
    "    try:\n",
    "        import mmcv\n",
    "        from mmcv.ops import point_sample\n",
    "        print('âœ… MMCV is fully functional.')\n",
    "        mmcv_ok = True\n",
    "    except (ImportError, ModuleNotFoundError) as e:\n",
    "        print(f'âŒ MMCV Error: {e}')\n",
    "    except Exception as e:\n",
    "        print(f'âŒ Unknown MMCV Error: {e}')\n",
    "\n",
    "    if mmcv_ok: return True\n",
    "\n",
    "    # Logic determines remediation\n",
    "    print('ðŸ”„ Attempting Repair...')\n",
    "\n",
    "    # Check for bleeding edge torch\n",
    "    is_bleeding_edge = False\n",
    "    if '+' in torch_ver: \n",
    "        base_ver = torch_ver.split('+')[0]\n",
    "    else:\n",
    "        base_ver = torch_ver\n",
    "        \n",
    "    major, minor = map(int, base_ver.split('.')[:2])\n",
    "    if major >= 2 and minor >= 4:\n",
    "        is_bleeding_edge = True\n",
    "\n",
    "    if is_bleeding_edge:\n",
    "        print('âš ï¸ Bleeding-edge Torch (>=2.4) detected. BINARY WHEELS DO NOT EXIST.')\n",
    "        print('ðŸ›  Starting SOURCE COMPILATION (approx 5-10 mins)...')\n",
    "        \n",
    "        # Cleaning old\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 'mmcv'], check=False)\n",
    "        \n",
    "        mmcv_dir = project_root / 'mmcv-source'\n",
    "        if mmcv_dir.exists(): shutil.rmtree(mmcv_dir)\n",
    "        \n",
    "        # Clone\n",
    "        subprocess.check_call(['git', 'clone', '-b', 'v2.2.0', 'https://github.com/open-mmlab/mmcv.git', str(mmcv_dir)])\n",
    "        \n",
    "        # Compile\n",
    "        env = os.environ.copy()\n",
    "        env['MMCV_WITH_OPS'] = '1'\n",
    "        env['FORCE_CUDA'] = '1'\n",
    "        env['MAX_JOBS'] = '8'\n",
    "        \n",
    "        print('â³ Compiling... check terminal for details if stuck.')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '.'], cwd=str(mmcv_dir), env=env)\n",
    "        print('âœ… Compilation Complete.')\n",
    "        shutil.rmtree(mmcv_dir)\n",
    "        return False\n",
    "    else:\n",
    "         print('â„¹ï¸ Standard Torch detected. Trying MIM install.')\n",
    "         subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', 'openmim'])\n",
    "         subprocess.check_call([sys.executable, '-m', 'mim', 'install', 'mmcv>=2.0.0'])\n",
    "         return False\n",
    "\n",
    "# --- 4. MMSEGMENTATION CHECK & REPAIR ---\n",
    "def check_mmseg():\n",
    "    print('ðŸ” Checking MMSegmentation installation...')\n",
    "    try:\n",
    "        import mmseg\n",
    "        print(f'   MMSeg version: {mmseg.__version__}')\n",
    "        try:\n",
    "            from mmseg.models.segmentors.mask2former import Mask2Former  # noqa: F401\n",
    "            print('âœ… Mask2Former segmentor is available.')\n",
    "            return True\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from mmseg.models.segmentors import Mask2Former  # noqa: F401\n",
    "                print('âœ… Mask2Former segmentor is available.')\n",
    "                return True\n",
    "            except ImportError:\n",
    "                from mmseg.models.segmentors import EncoderDecoder  # noqa: F401\n",
    "                from mmseg.models.decode_heads import Mask2FormerHead  # noqa: F401\n",
    "                print('â„¹ï¸ Mask2Former segmentor not exported by this mmseg build; using EncoderDecoder compatibility path.')\n",
    "                return True\n",
    "\n",
    "    except (ImportError, ModuleNotFoundError) as e:\n",
    "        print(f'âŒ MMSegmentation Issue: {e}')\n",
    "        print('ðŸ”„ Reinstalling MMSegmentation via MIM...')\n",
    "        \n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 'mmsegmentation'], check=False)\n",
    "        subprocess.check_call([sys.executable, '-m', 'mim', 'install', 'mmsegmentation>=1.2.2'])\n",
    "        print('âœ… Reinstall complete. PLEASE RESTART KERNEL.')\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸ Unknown MMSeg error: {e}')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# --- 5. SAFETY NOTE ---\n",
    "def patch_assertions():\n",
    "    print('â„¹ï¸ Skipping site-packages patching for mmseg/__init__.py (safer and reproducible).')\n",
    "    print('   If mmcv/mmseg versions are incompatible, reinstall matching versions instead of editing package files.')\n",
    "    return True\n",
    "\n",
    "if nuclear_cuda_fix() and match_mmcv_to_torch():\n",
    "    if check_mmseg():\n",
    "        patch_assertions()\n",
    "        print('ðŸš€ System Ready.')\n",
    "    else:\n",
    "        print('\\nâš ï¸  MMSEG UPDATED. PLEASE RESTART KERNEL.')\n",
    "else:\n",
    "    print('\\nâš ï¸  ENVIRONMENT UPDATED. PLEASE RESTART KERNEL.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ 1. Dataset Preview\n",
    "\n",
    "Visualize the **OmniCity** satellite imagery and ground truth **Masks** + **Surface Normals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_dataset(num_samples=3):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    \n",
    "    data_path = project_root / 'data/OmniCity'\n",
    "    train_file = data_path / 'train.txt'\n",
    "    \n",
    "    if not train_file.exists():\n",
    "        print(f'âŒ Multi-task training data not found at {data_path}.')\n",
    "        return\n",
    "        \n",
    "    with open(train_file, 'r') as f:\n",
    "        sample_ids = [line.strip() for line in f.readlines()[:num_samples]]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
    "    for i, sid in enumerate(sample_ids):\n",
    "        img = cv2.cvtColor(cv2.imread(str(data_path / 'images' / (sid + '.jpg'))), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(str(data_path / 'masks' / (sid + '.png')), cv2.IMREAD_UNCHANGED)\n",
    "        mask_vis = cv2.applyColorMap(((mask % 20) * 12).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        \n",
    "        axes[i, 0].imshow(img); axes[i, 0].set_title(sid); axes[i, 0].axis('off')\n",
    "        axes[i, 1].imshow(mask_vis); axes[i, 1].set_title('Mask'); axes[i, 1].axis('off')\n",
    "        \n",
    "        norm_path = data_path / 'normals' / (sid + '.npy')\n",
    "        if norm_path.exists():\n",
    "            normals = np.load(str(norm_path))\n",
    "            axes[i, 2].imshow(((normals + 1) * 127.5).astype(np.uint8))\n",
    "        axes[i, 2].set_title('Normals'); axes[i, 2].axis('off')\n",
    "        \n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "preview_dataset(num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ 2. Absolute Ideal SOTA Training Configuration\n",
    "\n",
    "Resuming from `iter_40000.pth` with **Physical Consistency Refinements**:\n",
    "- **Config**: `deeproof_production_swin_L.py` (300 Queries, MSTrain, ShadowAug)\n",
    "- **Geometry Loss**: `PhysicallyWeightedNormalLoss` (Azimuth Stability)\n",
    "- **Vectorization**: Global Dominant Orientation Snapping (CAD-ready)\n",
    "- **Load From**: `iter_40000.pth` (Baseline weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "from mmseg.utils import register_all_modules\n",
    "\n",
    "# Ensure mmseg registries/default scope are available in this kernel.\n",
    "register_all_modules(init_default_scope=False)\n",
    "\n",
    "# Explicitly import custom modules so they are registered before Runner build.\n",
    "import deeproof.models.backbones.swin_v2_compat\n",
    "import deeproof.models.deeproof_model\n",
    "import deeproof.models.heads.mask2former_head\n",
    "import deeproof.models.heads.geometry_head\n",
    "import deeproof.models.losses\n",
    "# FIX: Must import dataset module so DeepRoofDataset is registered in DATASETS registry\n",
    "# Without this, Runner.build() raises KeyError: DeepRoofDataset not found\n",
    "import deeproof.datasets.roof_dataset\n",
    "\n",
    "CONFIG_PATH = str(project_root / 'configs/deeproof_production_swin_L.py')\n",
    "# FIX: Updated work dir to v3 (fresh run with all bug fixes applied)\n",
    "WORK_DIR = str(project_root / 'work_dirs/deeproof_absolute_ideal_v1')\n",
    "\n",
    "cfg = Config.fromfile(CONFIG_PATH)\n",
    "cfg.default_scope = 'mmseg'\n",
    "\n",
    "# Safety: keep Swin window size compatible with loaded pretrained weights.\n",
    "if cfg.model.get('backbone', {}).get('window_size', None) not in (None, 12):\n",
    "    cfg.model.backbone.window_size = 12\n",
    "\n",
    "# Safety: SegDataPreProcessor.train needs exactly one of size/size_divisor.\n",
    "dp = cfg.model.setdefault('data_preprocessor', {})\n",
    "has_size = dp.get('size') is not None\n",
    "has_div = dp.get('size_divisor') is not None\n",
    "if has_size and has_div:\n",
    "    dp.pop('size', None)\n",
    "elif (not has_size) and (not has_div):\n",
    "    dp['size_divisor'] = 32\n",
    "\n",
    "# Safety: normalize loop keys if a stale/partially-merged config is loaded.\n",
    "if cfg.train_cfg.get('type') == 'EpochBasedTrainLoop' and 'max_iters' in cfg.train_cfg:\n",
    "    cfg.train_cfg.pop('max_iters')\n",
    "if cfg.train_cfg.get('type') == 'IterBasedTrainLoop' and 'max_epochs' in cfg.train_cfg:\n",
    "    cfg.train_cfg.pop('max_epochs')\n",
    "\n",
    "# Safety: mmengine PolyLR expects eta_min in this runtime.\n",
    "for sch in cfg.get('param_scheduler', []):\n",
    "    if isinstance(sch, dict) and sch.get('type') == 'PolyLR' and 'min_lr' in sch and 'eta_min' not in sch:\n",
    "        sch['eta_min'] = sch.pop('min_lr')\n",
    "\n",
    "# Safety: load_from should point to best iter_5000 checkpoint from previous run.\n",
    "# Update this path if your best checkpoint has a different location.\n",
    "import os\n",
    "best_ckpt_candidates = [\n",
    "    str(project_root / 'work_dirs/swin_l_finetune_v2/best_mIoU_iter_5000.pth'),\n",
    "    str(project_root / 'work_dirs/swin_l_scratch_v1/iter_40000.pth'),\n",
    "]\n",
    "for ckpt in best_ckpt_candidates:\n",
    "    if os.path.exists(ckpt):\n",
    "        # cfg.load_from = \n",
    "cfg.load_from = '/workspace/roof/work_dirs/swin_l_finetune_v3/best_mIoU_iter_25000.pth'ckpt\n",
    "        print(f'Checkpoint found: {ckpt}')\n",
    "        break\n",
    "else:\n",
    "    print('WARNING: No checkpoint found! Training will start from random init.')\n",
    "\n",
    "cfg.work_dir = WORK_DIR\n",
    "cfg.data_root = str(project_root / 'data/OmniCity/')\n",
    "cfg.train_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.val_dataloader.dataset.data_root = cfg.data_root\n",
    "\n",
    "loop_type = cfg.train_cfg.get('type', 'UnknownLoop')\n",
    "if loop_type == 'IterBasedTrainLoop':\n",
    "    train_span = f\"{cfg.train_cfg.max_iters} iters\"\n",
    "    report_interval = f\"Every {cfg.train_cfg.get('val_interval', 'N/A')} iters\"\n",
    "else:\n",
    "    train_span = f\"{cfg.train_cfg.max_epochs} epochs\"\n",
    "    report_interval = f\"Every {cfg.train_cfg.get('val_interval', 'N/A')} epoch(s)\"\n",
    "\n",
    "# Extract LR from optim_wrapper (MMEngine uses optim_wrapper, not bare optimizer)\n",
    "lr = cfg.optim_wrapper.optimizer.lr\n",
    "\n",
    "cls_w = cfg.model.decode_head.loss_cls.class_weight\n",
    "geo_w = cfg.model.geometry_loss_weight\n",
    "clip = cfg.optim_wrapper.get('clip_grad', {}).get('max_norm', 'N/A')\n",
    "\n",
    "print('SOTA CONFIG LOADED (Absolute Ideal - Physically Weighted Loss)')\n",
    "print(f'Training Span:        {train_span}')\n",
    "print(f'Initial LR:           {lr}')\n",
    "print(f'Gradient Clip Norm:   {clip}  (was 0.01 -> now 1.0)')\n",
    "print(f'Validation Interval:  {report_interval}')\n",
    "print(f'Batch Size:           {cfg.train_dataloader.batch_size}')\n",
    "print(f'Load From:            {cfg.get(\"load_from\", \"None\")}')\n",
    "print(f'Work Dir:             {WORK_DIR}')\n",
    "print(f'Class Weights:        bg={cls_w[0]}, flat={cls_w[1]}, sloped={cls_w[2]}, no_obj={cls_w[3]}')\n",
    "print(f'Geometry Loss Weight: {geo_w}  (was 10.0 -> now 2.0)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ 3. Kickoff Training\n",
    "\n",
    "This will invoke the `mmengine.Runner` and begin fine-tuning from `iter_40000.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "print(f'Starting Fine-Tune on: {device_name}')\n",
    "print(f'Batch size:            {cfg.train_dataloader.batch_size}')\n",
    "print(f'Work dir:              {cfg.work_dir}')\n",
    "print(f'Sloped class weight:   {cfg.model.decode_head.loss_cls.class_weight[2]}x')\n",
    "print(f'Geometry loss weight:  {cfg.model.geometry_loss_weight}')\n",
    "print(f'Gradient clip norm:    {cfg.optim_wrapper.get(\"clip_grad\", {}).get(\"max_norm\", \"N/A\")}')\n",
    "print()\n",
    "\n",
    "runner = Runner.from_cfg(cfg)\n",
    "runner.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š 4. Monitoring & Metrics\n",
    "\n",
    "Run this cell during or after training to visualize performance trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_logs(log_path):\n",
    "    \"\"\"Plot all losses including loss_geometry and mIoU from MMEngine JSON log.\"\"\"\n",
    "    if not os.path.exists(log_path):\n",
    "        print('No logs found yet.')\n",
    "        return\n",
    "\n",
    "    # Separate tracking for train metrics and val metrics\n",
    "    train_data = {'iter': [], 'loss': [], 'loss_cls': [], 'loss_mask': [], 'loss_dice': [], 'loss_geometry': []}\n",
    "    val_data = {'iter': [], 'mIoU': []}\n",
    "\n",
    "    with open(log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            # Training step metrics\n",
    "            if 'loss' in data and 'mode' not in data:\n",
    "                train_data['iter'].append(data.get('iter', 0))\n",
    "                train_data['loss'].append(data.get('loss', 0))\n",
    "                for k in ['loss_cls', 'loss_mask', 'loss_dice', 'loss_geometry']:\n",
    "                    if k in data:\n",
    "                        train_data[k].append(data[k])\n",
    "            # Validation metrics (mode=val)\n",
    "            if data.get('mode') == 'val' or 'mIoU' in data:\n",
    "                val_data['iter'].append(data.get('iter', data.get('step', 0)))\n",
    "                val_data['mIoU'].append(data.get('mIoU', 0))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Plot 1: Total loss\n",
    "    if train_data['iter']:\n",
    "        axes[0].plot(train_data['iter'], train_data['loss'], color='#e74c3c', linewidth=1.5, label='Total Loss')\n",
    "        axes[0].set_title('Total Loss')\n",
    "        axes[0].set_xlabel('Iteration')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: Individual losses\n",
    "    colors = {'loss_cls': '#3498db', 'loss_mask': '#2ecc71', 'loss_dice': '#f39c12', 'loss_geometry': '#9b59b6'}\n",
    "    for k, color in colors.items():\n",
    "        if train_data[k]:\n",
    "            axes[1].plot(train_data['iter'][:len(train_data[k])], train_data[k], color=color, linewidth=1.5, label=k, alpha=0.8)\n",
    "    axes[1].set_title('Individual Losses')\n",
    "    axes[1].set_xlabel('Iteration')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend(fontsize=8)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 3: mIoU\n",
    "    if val_data['iter']:\n",
    "        axes[2].plot(val_data['iter'], val_data['mIoU'], color='#27ae60', marker='o', linewidth=2, label='mIoU')\n",
    "        best_idx = val_data['mIoU'].index(max(val_data['mIoU']))\n",
    "        axes[2].axvline(val_data['iter'][best_idx], color='red', linestyle='--', alpha=0.7,\n",
    "                        label=f\"Best: {val_data['mIoU'][best_idx]:.3f} @ iter {val_data['iter'][best_idx]}\")\n",
    "        axes[2].set_title('Validation mIoU')\n",
    "        axes[2].set_xlabel('Iteration')\n",
    "        axes[2].set_ylabel('mIoU')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(alpha=0.3)\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'No validation data yet', ha='center', va='center', transform=axes[2].transAxes)\n",
    "\n",
    "    plt.suptitle(f'Training Monitor: {os.path.basename(log_path)}', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(WORK_DIR, 'training_monitor.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f'Plot saved to {WORK_DIR}/training_monitor.png')\n",
    "\n",
    "# FIX: MMEngine logs are in {work_dir}/TIMESTAMP/vis_data/scalars.json\n",
    "# Try both standard paths\n",
    "log_candidates = (\n",
    "    glob.glob(os.path.join(WORK_DIR, '*/vis_data/scalars.json')) +\n",
    "    glob.glob(os.path.join(WORK_DIR, 'vis_data/scalars.json'))\n",
    ")\n",
    "if log_candidates:\n",
    "    log_path = sorted(log_candidates)[-1]  # Most recent\n",
    "    print(f'Reading log: {log_path}')\n",
    "    plot_training_logs(log_path)\n",
    "else:\n",
    "    print('No logs found yet - run training first.')\n",
    "    print(f'Expected location: {WORK_DIR}/TIMESTAMP/vis_data/scalars.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
