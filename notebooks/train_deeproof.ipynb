{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6b9f42",
   "metadata": {},
   "source": [
    "# DeepRoof Training Notebook (Reproducible)\n",
    "\n",
    "Этот ноутбук специально сделан тонким: без патчей site-packages и без скрытых runtime-фиксов.\n",
    "Использует тот же конфиг и код-путь, что и CLI training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_project_root() -> Path:\n",
    "    c = Path.cwd().resolve()\n",
    "    for cand in [c, *c.parents]:\n",
    "        if (cand / 'configs').exists() and (cand / 'deeproof').exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError('Could not auto-detect project root')\n",
    "\n",
    "project_root = detect_project_root()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f'Project root: {project_root}')\n",
    "print(f'Python: {sys.executable}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf78cf",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Поменяй только параметры ниже. Остальная логика должна совпадать с production-конфигом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmseg.utils import register_all_modules\n",
    "from deeproof.utils.runtime_compat import apply_runtime_compat\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "import deeproof.models.backbones.swin_v2_compat\n",
    "import deeproof.models.deeproof_model\n",
    "import deeproof.models.heads.mask2former_head\n",
    "import deeproof.models.heads.geometry_head\n",
    "import deeproof.models.heads.dense_normal_head\n",
    "import deeproof.models.heads.edge_head\n",
    "import deeproof.models.losses\n",
    "import deeproof.datasets.roof_dataset\n",
    "import deeproof.datasets.universal_roof_dataset\n",
    "import deeproof.evaluation.metrics\n",
    "\n",
    "register_all_modules(init_default_scope=False)\n",
    "\n",
    "CONFIG_PATH = project_root / 'configs' / 'deeproof_production_swin_L.py'\n",
    "WORK_DIR = project_root / 'work_dirs' / 'deeproof_absolute_ideal_v1'\n",
    "DATA_ROOT = project_root / 'data' / 'OmniCity'\n",
    "AUTO_RESUME_LATEST = True\n",
    "AUTO_WARMSTART_BEST = False\n",
    "REQUIRE_COMPATIBLE_AUX_HEADS = True\n",
    "FALLBACK_TO_LOAD_IF_RESUME_INCOMPATIBLE = True\n",
    "DISABLE_BACKBONE_PRETRAIN_IF_LOADING = True\n",
    "\n",
    "REQUIRED_STATE_PREFIXES = [\n",
    "    'dense_geometry_head.',\n",
    "    'edge_head.',\n",
    "]\n",
    "\n",
    "\n",
    "def _safe_torch_load(path: Path):\n",
    "    # Explicit weights_only keeps behavior stable as PyTorch changes defaults.\n",
    "    try:\n",
    "        return torch.load(str(path), map_location='cpu', weights_only=False)\n",
    "    except TypeError:\n",
    "        return torch.load(str(path), map_location='cpu')\n",
    "\n",
    "\n",
    "def _read_last_checkpoint(work_dir: Path):\n",
    "    marker = work_dir / 'last_checkpoint'\n",
    "    if not marker.exists():\n",
    "        return None\n",
    "    raw = marker.read_text(encoding='utf-8').strip()\n",
    "    if not raw:\n",
    "        return None\n",
    "    ckpt = Path(raw)\n",
    "    if not ckpt.is_absolute():\n",
    "        ckpt = (work_dir / ckpt).resolve()\n",
    "    return ckpt if ckpt.exists() else None\n",
    "\n",
    "\n",
    "def _extract_state_dict(ckpt_obj):\n",
    "    if isinstance(ckpt_obj, dict):\n",
    "        sd = ckpt_obj.get('state_dict', ckpt_obj)\n",
    "        if isinstance(sd, dict):\n",
    "            return sd\n",
    "    return {}\n",
    "\n",
    "\n",
    "def _checkpoint_has_required_prefixes(ckpt_path: Path, prefixes):\n",
    "    try:\n",
    "        ckpt_obj = _safe_torch_load(ckpt_path)\n",
    "    except Exception:\n",
    "        return False\n",
    "    state_dict = _extract_state_dict(ckpt_obj)\n",
    "    keys = tuple(state_dict.keys())\n",
    "    return all(any(k.startswith(prefix) for k in keys) for prefix in prefixes)\n",
    "\n",
    "\n",
    "cfg = Config.fromfile(str(CONFIG_PATH))\n",
    "apply_runtime_compat(cfg)\n",
    "cfg.default_scope = 'mmseg'\n",
    "cfg.work_dir = str(WORK_DIR)\n",
    "\n",
    "if cfg.get('train_dataloader') and cfg.train_dataloader.get('dataset'):\n",
    "    ds = cfg.train_dataloader.dataset\n",
    "    if ds.get('type') == 'DeepRoofDataset':\n",
    "        ds.data_root = str(DATA_ROOT)\n",
    "if cfg.get('val_dataloader') and cfg.val_dataloader.get('dataset'):\n",
    "    ds = cfg.val_dataloader.dataset\n",
    "    if ds.get('type') == 'DeepRoofDataset':\n",
    "        ds.data_root = str(DATA_ROOT)\n",
    "\n",
    "selected_mode = 'fresh'\n",
    "selected_ckpt = None\n",
    "selection_reason = 'no checkpoint selected'\n",
    "\n",
    "if AUTO_RESUME_LATEST:\n",
    "    last_ckpt = _read_last_checkpoint(WORK_DIR)\n",
    "    if last_ckpt is not None:\n",
    "        compatible = (not REQUIRE_COMPATIBLE_AUX_HEADS) or _checkpoint_has_required_prefixes(last_ckpt, REQUIRED_STATE_PREFIXES)\n",
    "        if compatible:\n",
    "            cfg.resume = True\n",
    "            cfg.load_from = None\n",
    "            selected_mode = 'resume_latest'\n",
    "            selected_ckpt = last_ckpt\n",
    "            selection_reason = 'last checkpoint is architecture-compatible; full resume enabled'\n",
    "        elif FALLBACK_TO_LOAD_IF_RESUME_INCOMPATIBLE:\n",
    "            cfg.resume = False\n",
    "            cfg.load_from = str(last_ckpt)\n",
    "            selected_mode = 'warmstart_last_incompatible'\n",
    "            selected_ckpt = last_ckpt\n",
    "            selection_reason = 'last checkpoint missing new heads; switched to weights-only load_from'\n",
    "        else:\n",
    "            selection_reason = 'last checkpoint is incompatible and fallback is disabled'\n",
    "\n",
    "if selected_mode == 'fresh' and AUTO_WARMSTART_BEST:\n",
    "    best_ckpts = sorted(WORK_DIR.glob('best_mIoU*.pth'), reverse=True)\n",
    "    for ckpt in best_ckpts:\n",
    "        compatible = (not REQUIRE_COMPATIBLE_AUX_HEADS) or _checkpoint_has_required_prefixes(ckpt, REQUIRED_STATE_PREFIXES)\n",
    "        if compatible:\n",
    "            cfg.load_from = str(ckpt)\n",
    "            cfg.resume = False\n",
    "            selected_mode = 'warmstart_best'\n",
    "            selected_ckpt = ckpt\n",
    "            selection_reason = 'compatible best checkpoint selected for weights-only warmstart'\n",
    "            break\n",
    "\n",
    "if DISABLE_BACKBONE_PRETRAIN_IF_LOADING and (selected_mode in ('resume_latest', 'warmstart_best', 'warmstart_last_incompatible')):\n",
    "    if cfg.get('model') and cfg.model.get('backbone'):\n",
    "        cfg.model.backbone.init_cfg = None\n",
    "\n",
    "if cfg.get('val_dataloader') is not None and cfg.get('val_evaluator') is not None and cfg.get('val_cfg') is None:\n",
    "    cfg.val_cfg = dict(type='ValLoop')\n",
    "if cfg.get('test_dataloader') is not None and cfg.get('test_evaluator') is not None and cfg.get('test_cfg') is None:\n",
    "    cfg.test_cfg = dict(type='TestLoop')\n",
    "\n",
    "print('Config loaded')\n",
    "print('work_dir:', cfg.work_dir)\n",
    "print('checkpoint_mode:', selected_mode)\n",
    "print('checkpoint_path:', str(selected_ckpt) if selected_ckpt else 'None')\n",
    "print('checkpoint_reason:', selection_reason)\n",
    "print('resume:', cfg.get('resume', False))\n",
    "print('load_from:', cfg.get('load_from', 'None'))\n",
    "print('batch_size:', cfg.train_dataloader.batch_size)\n",
    "print('max_iters:', cfg.train_cfg.get('max_iters', 'N/A'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235aa394",
   "metadata": {},
   "source": [
    "## Optional Dataset Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preview_dataset(num_samples=2):\n",
    "    train_txt = Path(cfg.train_dataloader.dataset.ann_file)\n",
    "    if not train_txt.is_absolute():\n",
    "        train_txt = Path(cfg.train_dataloader.dataset.data_root) / train_txt\n",
    "    if not train_txt.exists():\n",
    "        print('train.txt not found:', train_txt)\n",
    "        return\n",
    "\n",
    "    sample_ids = [x.strip() for x in train_txt.read_text(encoding='utf-8').splitlines() if x.strip()][:num_samples]\n",
    "    data_root = Path(cfg.train_dataloader.dataset.data_root)\n",
    "\n",
    "    fig, axes = plt.subplots(len(sample_ids), 3, figsize=(15, 5 * len(sample_ids)))\n",
    "    if len(sample_ids) == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "\n",
    "    for i, sid in enumerate(sample_ids):\n",
    "        img = cv2.cvtColor(cv2.imread(str(data_root / 'images' / f'{sid}.jpg')), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(data_root / 'masks' / f'{sid}.png'), cv2.IMREAD_UNCHANGED)\n",
    "        norm_p = data_root / 'normals' / f'{sid}.npy'\n",
    "\n",
    "        axes[i,0].imshow(img); axes[i,0].set_title(f'image: {sid}'); axes[i,0].axis('off')\n",
    "        axes[i,1].imshow(mask, cmap='tab20'); axes[i,1].set_title('instance mask'); axes[i,1].axis('off')\n",
    "\n",
    "        if norm_p.exists():\n",
    "            n = np.load(str(norm_p))\n",
    "            n_vis = ((n + 1.0) * 127.5).clip(0,255).astype(np.uint8)\n",
    "            axes[i,2].imshow(n_vis)\n",
    "        else:\n",
    "            axes[i,2].text(0.5,0.5,'no normal file',ha='center',va='center')\n",
    "        axes[i,2].set_title('normals'); axes[i,2].axis('off')\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "preview_dataset(num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97b235",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "print('Device:', device_name)\n",
    "runner = Runner.from_cfg(cfg)\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b6d22",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_logs(work_dir):\n",
    "    candidates = glob.glob(os.path.join(work_dir, '*/vis_data/scalars.json')) + glob.glob(os.path.join(work_dir, 'vis_data/scalars.json'))\n",
    "    if not candidates:\n",
    "        print('No scalar logs found.')\n",
    "        return\n",
    "    log_path = sorted(candidates)[-1]\n",
    "    print('Reading:', log_path)\n",
    "\n",
    "    train_iter = []\n",
    "    losses = {}\n",
    "    val_iter = []\n",
    "    val_miou = []\n",
    "\n",
    "    with open(log_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                d = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if 'loss' in d and d.get('mode', 'train') != 'val':\n",
    "                it = d.get('iter', d.get('step', 0))\n",
    "                train_iter.append(it)\n",
    "                for k,v in d.items():\n",
    "                    if k.startswith('loss') and isinstance(v, (int,float)):\n",
    "                        losses.setdefault(k, []).append(float(v))\n",
    "\n",
    "            if d.get('mode') == 'val' and 'mIoU' in d:\n",
    "                val_iter.append(d.get('iter', d.get('step', 0)))\n",
    "                val_miou.append(float(d['mIoU']))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16,5))\n",
    "    if losses:\n",
    "        for k, vals in sorted(losses.items()):\n",
    "            x = train_iter[:len(vals)]\n",
    "            axes[0].plot(x, vals, label=k, linewidth=1.2)\n",
    "        axes[0].set_title('Train losses')\n",
    "        axes[0].set_xlabel('iter')\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        axes[0].legend(fontsize=8)\n",
    "\n",
    "    if val_iter:\n",
    "        axes[1].plot(val_iter, val_miou, marker='o', linewidth=2)\n",
    "        axes[1].set_title('Validation mIoU')\n",
    "        axes[1].set_xlabel('iter')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_training_logs(cfg.work_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
