{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6b9f42",
   "metadata": {},
   "source": [
    "# DeepRoof Training Notebook (Reproducible)\n",
    "\n",
    "Этот ноутбук специально сделан тонким: без патчей site-packages и без скрытых runtime-фиксов.\n",
    "Использует тот же конфиг и код-путь, что и CLI training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_project_root() -> Path:\n",
    "    c = Path.cwd().resolve()\n",
    "    for cand in [c, *c.parents]:\n",
    "        if (cand / 'configs').exists() and (cand / 'deeproof').exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError('Could not auto-detect project root')\n",
    "\n",
    "project_root = detect_project_root()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f'Project root: {project_root}')\n",
    "print(f'Python: {sys.executable}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf78cf",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Поменяй только параметры ниже. Остальная логика должна совпадать с production-конфигом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmseg.utils import register_all_modules\n",
    "from deeproof.utils.runtime_compat import apply_runtime_compat\n",
    "\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import deeproof.models.backbones.swin_v2_compat\n",
    "import deeproof.models.deeproof_model\n",
    "import deeproof.models.heads.mask2former_head\n",
    "import deeproof.models.heads.geometry_head\n",
    "import deeproof.models.heads.dense_normal_head\n",
    "import deeproof.models.heads.edge_head\n",
    "import deeproof.models.losses\n",
    "import deeproof.datasets.roof_dataset\n",
    "import deeproof.datasets.universal_roof_dataset\n",
    "import deeproof.evaluation.metrics\n",
    "import deeproof.hooks.progress_hook\n",
    "\n",
    "register_all_modules(init_default_scope=False)\n",
    "\n",
    "CONFIG_PATH = project_root / 'configs' / 'deeproof_production_swin_L.py'\n",
    "WORK_DIR_BASE = project_root / 'work_dirs'\n",
    "RUN_NAME = f\"deeproof_notebook_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "WORK_DIR = WORK_DIR_BASE / RUN_NAME\n",
    "DATA_ROOT = project_root / 'data' / 'OmniCity'\n",
    "FORCE_NEW_WORK_DIR = True\n",
    "AUTO_RESUME_LATEST = False\n",
    "AUTO_WARMSTART_BEST = False\n",
    "REQUIRE_COMPATIBLE_AUX_HEADS = True\n",
    "FALLBACK_TO_LOAD_IF_RESUME_INCOMPATIBLE = False\n",
    "DISABLE_BACKBONE_PRETRAIN_IF_LOADING = True\n",
    "SAFE_NOTEBOOK_DATALOADER = True\n",
    "RUN_DATALOADER_PREFLIGHT = True\n",
    "PREFLIGHT_NUM_SAMPLES = 16\n",
    "\n",
    "# Notebook compute-safe profile: keeps production config intact but makes\n",
    "# interactive runs stable/fast enough to pass iter=0 quickly.\n",
    "NOTEBOOK_COMPUTE_SAFE_PROFILE = True\n",
    "NOTEBOOK_BATCH_SIZE = 1\n",
    "NOTEBOOK_IMAGE_SIZE = (768, 768)\n",
    "NOTEBOOK_NUM_QUERIES = 150\n",
    "NOTEBOOK_NUM_POINTS = 4096\n",
    "NOTEBOOK_USE_AMP = True\n",
    "\n",
    "REQUIRED_STATE_PREFIXES = [\n",
    "    'dense_geometry_head.',\n",
    "    'edge_head.',\n",
    "]\n",
    "\n",
    "\n",
    "def _safe_torch_load(path: Path):\n",
    "    # Explicit weights_only keeps behavior stable as PyTorch changes defaults.\n",
    "    try:\n",
    "        return torch.load(str(path), map_location='cpu', weights_only=False)\n",
    "    except TypeError:\n",
    "        return torch.load(str(path), map_location='cpu')\n",
    "\n",
    "\n",
    "def _read_last_checkpoint(work_dir: Path):\n",
    "    marker = work_dir / 'last_checkpoint'\n",
    "    if not marker.exists():\n",
    "        return None\n",
    "    raw = marker.read_text(encoding='utf-8').strip()\n",
    "    if not raw:\n",
    "        return None\n",
    "    ckpt = Path(raw)\n",
    "    if not ckpt.is_absolute():\n",
    "        ckpt = (work_dir / ckpt).resolve()\n",
    "    return ckpt if ckpt.exists() else None\n",
    "\n",
    "\n",
    "def _extract_state_dict(ckpt_obj):\n",
    "    if isinstance(ckpt_obj, dict):\n",
    "        sd = ckpt_obj.get('state_dict', ckpt_obj)\n",
    "        if isinstance(sd, dict):\n",
    "            return sd\n",
    "    return {}\n",
    "\n",
    "\n",
    "def _checkpoint_has_required_prefixes(ckpt_path: Path, prefixes):\n",
    "    try:\n",
    "        ckpt_obj = _safe_torch_load(ckpt_path)\n",
    "    except Exception:\n",
    "        return False\n",
    "    state_dict = _extract_state_dict(ckpt_obj)\n",
    "    keys = tuple(state_dict.keys())\n",
    "    return all(any(k.startswith(prefix) for k in keys) for prefix in prefixes)\n",
    "\n",
    "\n",
    "cfg = Config.fromfile(str(CONFIG_PATH))\n",
    "apply_runtime_compat(cfg)\n",
    "cfg.default_scope = 'mmseg'\n",
    "\n",
    "# Silence IoUMetric prefix warning and keep save_best key stable.\n",
    "val_eval = cfg.get('val_evaluator', None)\n",
    "if isinstance(val_eval, dict):\n",
    "    if val_eval.get('type') == 'IoUMetric' and val_eval.get('prefix', None) is None:\n",
    "        val_eval['prefix'] = ''\n",
    "elif isinstance(val_eval, (list, tuple)):\n",
    "    for metric in val_eval:\n",
    "        if isinstance(metric, dict) and metric.get('type') == 'IoUMetric' and metric.get('prefix', None) is None:\n",
    "            metric['prefix'] = ''\n",
    "\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "cfg.work_dir = str(WORK_DIR)\n",
    "\n",
    "# Explicit notebook heartbeat hook (print-based), useful when logger output is buffered.\n",
    "cfg.custom_hooks = cfg.get('custom_hooks', [])\n",
    "has_progress = False\n",
    "for h in cfg.custom_hooks:\n",
    "    if isinstance(h, dict) and h.get('type') == 'DeepRoofProgressHook':\n",
    "        h['interval'] = 10\n",
    "        h['heartbeat_sec'] = 20\n",
    "        h['dataloader_warn_sec'] = 90\n",
    "        h['flush'] = True\n",
    "        has_progress = True\n",
    "if not has_progress:\n",
    "    cfg.custom_hooks.append(dict(type='DeepRoofProgressHook', interval=10, heartbeat_sec=20, dataloader_warn_sec=90, flush=True))\n",
    "\n",
    "\n",
    "\n",
    "# Ensure frequent, visible train logs in notebook.\n",
    "cfg.log_level = 'INFO'\n",
    "cfg.log_processor = dict(by_epoch=False, window_size=10)\n",
    "cfg.setdefault('default_hooks', {})\n",
    "if cfg.default_hooks.get('logger') is None:\n",
    "    cfg.default_hooks.logger = dict(type='LoggerHook', interval=10, log_metric_by_epoch=False)\n",
    "else:\n",
    "    cfg.default_hooks.logger.interval = 10\n",
    "    cfg.default_hooks.logger.log_metric_by_epoch = False\n",
    "\n",
    "\n",
    "# Notebook checkpoint policy: save early and often into this run folder.\n",
    "cfg.setdefault('default_hooks', {})\n",
    "if cfg.default_hooks.get('checkpoint') is None:\n",
    "    cfg.default_hooks.checkpoint = dict(type='CheckpointHook')\n",
    "cfg.default_hooks.checkpoint.by_epoch = False\n",
    "cfg.default_hooks.checkpoint.interval = 500\n",
    "cfg.default_hooks.checkpoint.save_best = 'mIoU'\n",
    "cfg.default_hooks.checkpoint.rule = 'greater'\n",
    "cfg.default_hooks.checkpoint.max_keep_ckpts = 5\n",
    "cfg.default_hooks.checkpoint.save_last = True\n",
    "if cfg.get('train_cfg') is not None:\n",
    "    cfg.train_cfg.val_interval = min(int(cfg.train_cfg.get('val_interval', 5000)), 500)\n",
    "\n",
    "if cfg.get('train_dataloader') and cfg.train_dataloader.get('dataset'):\n",
    "    ds = cfg.train_dataloader.dataset\n",
    "    if ds.get('type') == 'DeepRoofDataset':\n",
    "        ds.data_root = str(DATA_ROOT)\n",
    "if cfg.get('val_dataloader') and cfg.val_dataloader.get('dataset'):\n",
    "    ds = cfg.val_dataloader.dataset\n",
    "    if ds.get('type') == 'DeepRoofDataset':\n",
    "        ds.data_root = str(DATA_ROOT)\n",
    "\n",
    "\n",
    "if SAFE_NOTEBOOK_DATALOADER:\n",
    "    # Jupyter-safe mode: avoid worker startup deadlocks/hangs on first batch.\n",
    "    if cfg.get('train_dataloader') is not None:\n",
    "        cfg.train_dataloader.num_workers = 0\n",
    "        cfg.train_dataloader.persistent_workers = False\n",
    "        cfg.train_dataloader.timeout = 0\n",
    "        if 'prefetch_factor' in cfg.train_dataloader:\n",
    "            cfg.train_dataloader.pop('prefetch_factor')\n",
    "    if cfg.get('val_dataloader') is not None:\n",
    "        cfg.val_dataloader.num_workers = 0\n",
    "        cfg.val_dataloader.persistent_workers = False\n",
    "        cfg.val_dataloader.timeout = 0\n",
    "        if 'prefetch_factor' in cfg.val_dataloader:\n",
    "            cfg.val_dataloader.pop('prefetch_factor')\n",
    "\n",
    "if NOTEBOOK_COMPUTE_SAFE_PROFILE:\n",
    "    # Reduce per-iter compute so notebook training becomes responsive.\n",
    "    if cfg.get('train_dataloader') is not None:\n",
    "        cfg.train_dataloader.batch_size = int(max(1, NOTEBOOK_BATCH_SIZE))\n",
    "        if cfg.train_dataloader.get('dataset') is not None:\n",
    "            cfg.train_dataloader.dataset.image_size = tuple(NOTEBOOK_IMAGE_SIZE)\n",
    "    if cfg.get('val_dataloader') is not None and cfg.val_dataloader.get('dataset') is not None:\n",
    "        cfg.val_dataloader.dataset.image_size = tuple(NOTEBOOK_IMAGE_SIZE)\n",
    "\n",
    "    if cfg.get('model') is not None and cfg.model.get('decode_head') is not None:\n",
    "        cfg.model.decode_head.num_queries = int(max(64, NOTEBOOK_NUM_QUERIES))\n",
    "        if cfg.model.decode_head.get('train_cfg') is not None:\n",
    "            cfg.model.decode_head.train_cfg.num_points = int(max(1024, NOTEBOOK_NUM_POINTS))\n",
    "\n",
    "    if NOTEBOOK_USE_AMP and cfg.get('optim_wrapper') is not None:\n",
    "        ow = cfg.optim_wrapper\n",
    "        if ow.get('type', 'OptimWrapper') != 'AmpOptimWrapper':\n",
    "            amp_wrapper = dict(\n",
    "                type='AmpOptimWrapper',\n",
    "                optimizer=ow.optimizer,\n",
    "                loss_scale='dynamic')\n",
    "            if ow.get('clip_grad', None) is not None:\n",
    "                amp_wrapper['clip_grad'] = ow.clip_grad\n",
    "            if ow.get('paramwise_cfg', None) is not None:\n",
    "                amp_wrapper['paramwise_cfg'] = ow.paramwise_cfg\n",
    "            cfg.optim_wrapper = amp_wrapper\n",
    "\n",
    "if RUN_DATALOADER_PREFLIGHT:\n",
    "    import time\n",
    "    from torch.utils.data import DataLoader\n",
    "    from mmengine.dataset import pseudo_collate\n",
    "    from mmseg.registry import DATASETS\n",
    "\n",
    "    t0 = time.time()\n",
    "    train_ds = DATASETS.build(cfg.train_dataloader.dataset)\n",
    "    ds_len = len(train_ds)\n",
    "    print('preflight_train_len:', ds_len)\n",
    "    if ds_len <= 0:\n",
    "        raise RuntimeError('Train dataset is empty. Check ann_file/data_root paths.')\n",
    "\n",
    "    n_check = min(int(PREFLIGHT_NUM_SAMPLES), ds_len)\n",
    "    t_fetch = time.time()\n",
    "    first_keys = None\n",
    "    max_instances_seen = 0\n",
    "    for i in range(n_check):\n",
    "        try:\n",
    "            smp = train_ds[i]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f'Preflight failed at sample index {i}: {e}') from e\n",
    "        if first_keys is None:\n",
    "            first_keys = sorted(list(smp.keys()))\n",
    "        try:\n",
    "            n_inst = int(getattr(smp['data_samples'].gt_instances, 'labels').shape[0])\n",
    "            max_instances_seen = max(max_instances_seen, n_inst)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print('preflight_first_sample_keys:', first_keys)\n",
    "    print('preflight_checked_samples:', n_check)\n",
    "    print('preflight_max_instances_seen:', max_instances_seen)\n",
    "    print(f'preflight_fetch_time_sec: {time.time() - t_fetch:.2f}')\n",
    "\n",
    "    # DataLoader smoke test: catches first-batch stalls before runner.train().\n",
    "    t_dl = time.time()\n",
    "    smoke_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=pseudo_collate,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    try:\n",
    "        first_batch = next(iter(smoke_loader))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'Dataloader preflight failed on first batch: {e}') from e\n",
    "    print('preflight_first_batch_type:', type(first_batch).__name__)\n",
    "    print(f'preflight_first_batch_time_sec: {time.time() - t_dl:.2f}')\n",
    "    print(f'preflight_total_time_sec: {time.time() - t0:.2f}')\n",
    "\n",
    "selected_mode = 'fresh'\n",
    "selected_ckpt = None\n",
    "selection_reason = 'no checkpoint selected'\n",
    "\n",
    "if AUTO_RESUME_LATEST:\n",
    "    last_ckpt = _read_last_checkpoint(WORK_DIR)\n",
    "    if last_ckpt is not None:\n",
    "        compatible = (not REQUIRE_COMPATIBLE_AUX_HEADS) or _checkpoint_has_required_prefixes(last_ckpt, REQUIRED_STATE_PREFIXES)\n",
    "        if compatible:\n",
    "            cfg.resume = True\n",
    "            cfg.load_from = None\n",
    "            selected_mode = 'resume_latest'\n",
    "            selected_ckpt = last_ckpt\n",
    "            selection_reason = 'last checkpoint is architecture-compatible; full resume enabled'\n",
    "        elif FALLBACK_TO_LOAD_IF_RESUME_INCOMPATIBLE:\n",
    "            cfg.resume = False\n",
    "            cfg.load_from = str(last_ckpt)\n",
    "            selected_mode = 'warmstart_last_incompatible'\n",
    "            selected_ckpt = last_ckpt\n",
    "            selection_reason = 'last checkpoint missing new heads; switched to weights-only load_from'\n",
    "        else:\n",
    "            selection_reason = 'last checkpoint is incompatible with current model heads; checkpoint load skipped (fresh start with configured pretrain only)'\n",
    "\n",
    "if selected_mode == 'fresh' and AUTO_WARMSTART_BEST:\n",
    "    best_ckpts = sorted(WORK_DIR.glob('best_mIoU*.pth'), reverse=True)\n",
    "    for ckpt in best_ckpts:\n",
    "        compatible = (not REQUIRE_COMPATIBLE_AUX_HEADS) or _checkpoint_has_required_prefixes(ckpt, REQUIRED_STATE_PREFIXES)\n",
    "        if compatible:\n",
    "            cfg.load_from = str(ckpt)\n",
    "            cfg.resume = False\n",
    "            selected_mode = 'warmstart_best'\n",
    "            selected_ckpt = ckpt\n",
    "            selection_reason = 'compatible best checkpoint selected for weights-only warmstart'\n",
    "            break\n",
    "\n",
    "if DISABLE_BACKBONE_PRETRAIN_IF_LOADING and (selected_mode in ('resume_latest', 'warmstart_best', 'warmstart_last_incompatible')):\n",
    "    if cfg.get('model') and cfg.model.get('backbone'):\n",
    "        cfg.model.backbone.init_cfg = None\n",
    "\n",
    "if cfg.get('val_dataloader') is not None and cfg.get('val_evaluator') is not None and cfg.get('val_cfg') is None:\n",
    "    cfg.val_cfg = dict(type='ValLoop')\n",
    "if cfg.get('test_dataloader') is not None and cfg.get('test_evaluator') is not None and cfg.get('test_cfg') is None:\n",
    "    cfg.test_cfg = dict(type='TestLoop')\n",
    "\n",
    "print('Config loaded')\n",
    "print('work_dir:', cfg.work_dir)\n",
    "print('checkpoint_mode:', selected_mode)\n",
    "print('checkpoint_path:', str(selected_ckpt) if selected_ckpt else 'None')\n",
    "print('checkpoint_reason:', selection_reason)\n",
    "print('resume:', cfg.get('resume', False))\n",
    "print('load_from:', cfg.get('load_from', 'None'))\n",
    "print('batch_size:', cfg.train_dataloader.batch_size)\n",
    "print('max_iters:', cfg.train_cfg.get('max_iters', 'N/A'))\n",
    "\n",
    "print('logger_interval:', cfg.default_hooks.get('logger', {}).get('interval', 'N/A'))\n",
    "print('log_by_epoch:', cfg.log_processor.get('by_epoch', 'N/A'))\n",
    "\n",
    "print('custom_hooks:', cfg.get('custom_hooks', []))\n",
    "print('train_num_workers:', cfg.train_dataloader.get('num_workers', 'N/A'))\n",
    "print('train_persistent_workers:', cfg.train_dataloader.get('persistent_workers', 'N/A'))\n",
    "print('run_name:', RUN_NAME)\n",
    "print('checkpoint_interval:', cfg.default_hooks.get('checkpoint', {}).get('interval', 'N/A'))\n",
    "print('val_interval:', cfg.train_cfg.get('val_interval', 'N/A'))\n",
    "print('compute_safe_profile:', NOTEBOOK_COMPUTE_SAFE_PROFILE)\n",
    "print('batch_size_effective:', cfg.train_dataloader.get('batch_size', 'N/A'))\n",
    "print('image_size_effective:', cfg.train_dataloader.dataset.get('image_size', 'N/A'))\n",
    "print('num_queries_effective:', cfg.model.decode_head.get('num_queries', 'N/A'))\n",
    "print('num_points_effective:', cfg.model.decode_head.train_cfg.get('num_points', 'N/A'))\n",
    "print('optim_wrapper_type:', cfg.optim_wrapper.get('type', 'N/A'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235aa394",
   "metadata": {},
   "source": [
    "## Optional Dataset Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preview_dataset(num_samples=2):\n",
    "    train_txt = Path(cfg.train_dataloader.dataset.ann_file)\n",
    "    if not train_txt.is_absolute():\n",
    "        train_txt = Path(cfg.train_dataloader.dataset.data_root) / train_txt\n",
    "    if not train_txt.exists():\n",
    "        print('train.txt not found:', train_txt)\n",
    "        return\n",
    "\n",
    "    sample_ids = [x.strip() for x in train_txt.read_text(encoding='utf-8').splitlines() if x.strip()][:num_samples]\n",
    "    data_root = Path(cfg.train_dataloader.dataset.data_root)\n",
    "\n",
    "    fig, axes = plt.subplots(len(sample_ids), 3, figsize=(15, 5 * len(sample_ids)))\n",
    "    if len(sample_ids) == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "\n",
    "    for i, sid in enumerate(sample_ids):\n",
    "        img = cv2.cvtColor(cv2.imread(str(data_root / 'images' / f'{sid}.jpg')), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(data_root / 'masks' / f'{sid}.png'), cv2.IMREAD_UNCHANGED)\n",
    "        norm_p = data_root / 'normals' / f'{sid}.npy'\n",
    "\n",
    "        axes[i,0].imshow(img); axes[i,0].set_title(f'image: {sid}'); axes[i,0].axis('off')\n",
    "        axes[i,1].imshow(mask, cmap='tab20'); axes[i,1].set_title('instance mask'); axes[i,1].axis('off')\n",
    "\n",
    "        if norm_p.exists():\n",
    "            n = np.load(str(norm_p))\n",
    "            n_vis = ((n + 1.0) * 127.5).clip(0,255).astype(np.uint8)\n",
    "            axes[i,2].imshow(n_vis)\n",
    "        else:\n",
    "            axes[i,2].text(0.5,0.5,'no normal file',ha='center',va='center')\n",
    "        axes[i,2].set_title('normals'); axes[i,2].axis('off')\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "preview_dataset(num_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97b235",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "print('Device:', device_name)\n",
    "print('Training work_dir:', cfg.work_dir)\n",
    "Path(cfg.work_dir).mkdir(parents=True, exist_ok=True)\n",
    "(Path(cfg.work_dir) / 'RUN_STARTED.txt').write_text(\n",
    "    f'started_at={datetime.now().isoformat()}\\n', encoding='utf-8')\n",
    "\n",
    "runner = Runner.from_cfg(cfg)\n",
    "runner.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b6d22",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_logs(work_dir):\n",
    "    candidates = glob.glob(os.path.join(work_dir, '*/vis_data/scalars.json')) + glob.glob(os.path.join(work_dir, 'vis_data/scalars.json'))\n",
    "    if not candidates:\n",
    "        print('No scalar logs found.')\n",
    "        return\n",
    "    log_path = sorted(candidates)[-1]\n",
    "    print('Reading:', log_path)\n",
    "\n",
    "    train_iter = []\n",
    "    losses = {}\n",
    "    val_iter = []\n",
    "    val_miou = []\n",
    "\n",
    "    with open(log_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                d = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if 'loss' in d and d.get('mode', 'train') != 'val':\n",
    "                it = d.get('iter', d.get('step', 0))\n",
    "                train_iter.append(it)\n",
    "                for k,v in d.items():\n",
    "                    if k.startswith('loss') and isinstance(v, (int,float)):\n",
    "                        losses.setdefault(k, []).append(float(v))\n",
    "\n",
    "            if d.get('mode') == 'val' and 'mIoU' in d:\n",
    "                val_iter.append(d.get('iter', d.get('step', 0)))\n",
    "                val_miou.append(float(d['mIoU']))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16,5))\n",
    "    if losses:\n",
    "        for k, vals in sorted(losses.items()):\n",
    "            x = train_iter[:len(vals)]\n",
    "            axes[0].plot(x, vals, label=k, linewidth=1.2)\n",
    "        axes[0].set_title('Train losses')\n",
    "        axes[0].set_xlabel('iter')\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        axes[0].legend(fontsize=8)\n",
    "\n",
    "    if val_iter:\n",
    "        axes[1].plot(val_iter, val_miou, marker='o', linewidth=2)\n",
    "        axes[1].set_title('Validation mIoU')\n",
    "        axes[1].set_xlabel('iter')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_training_logs(cfg.work_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
